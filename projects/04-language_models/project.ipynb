{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Language Models 🗣\n",
    "\n",
    "### Due Date: Monday, Dec 1st at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Project 4! Project 4 will work similar to Projects 1 and 2, in that:\n",
    "- All of your code should go in `project.py`.\n",
    "- As a thanksgiving gift, there is **no checkpoint this time!** \n",
    "\n",
    "### Working with a Partner 👯\n",
    "\n",
    "You may work together on projects (and projects only!) with a partner. If you work with a partner, you are both required to actively contribute to all parts of the project. You must both be working on the assignment at the same time together, either physically or virtually on a Zoom call. You are encouraged to follow the pair programming model, in which you work on just a single computer and alternate who writes the code and who thinks about the problems at a high level.\n",
    "\n",
    "In particular, you **cannot** split up the project and each work on separate parts independently.\n",
    "\n",
    "Note that if you do work with a partner, you and your partner must submit the whole project together. See [here](https://dsc80.com/syllabus/#projects) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Assignment\n",
    "\n",
    "### Introduction to Language Models (LMs)\n",
    "\n",
    "You've almost certainly heard about ChatGPT, a large language model developed by OpenAI. It is designed to generate human-like responses to natural language input.\n",
    "\n",
    "<center><img src=\"imgs/chatgpt.png\" width=60%>Inception!</center>\n",
    "\n",
    "In this project, you will build your own [language models](https://en.wikipedia.org/wiki/Language_model), using publicly-available books found on [Project Gutenberg](https://www.gutenberg.org). Language models attempt to capture the likelihood that a given sequence of words occur in a given collection of texts, known as a \"corpus.\" For instance, a language model can assign probabilities to the sentences `'when I drink Coke I smile'` and `'when I drink pizza I smile'`. One would expect the former sentence to have a higher probability than the former 😊.\n",
    "\n",
    "As with all statistical models, the true data generating process is unknown to us, so all we can do is **estimate** the probabilities of sentences. For example, one might estimate the probability of a sentence as simply the product of the empirical probabilities (i.e., the number of times a word is observed in a dataset divided by the number of words in that dataset). In the above example, we may have:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile})$$\n",
    "\n",
    "We'll look at several estimation methods in this project (including the one above); as you'll see, some are more sophisticated than others. Don't worry – we'll re-introduce all of the relevant mathematics as it becomes necessary throughout the project.\n",
    "\n",
    "### Tokenizing a Corpus\n",
    "\n",
    "To train a language model on a corpus, we must break the corpus into a sequence of words. In reality though, the sequences are not made up entirely of words, but rather more general terms called *tokens*, which may include punctuation and other terms. The process of separating a corpus into its tokens is called **tokenization**.\n",
    "\n",
    "Below are a few examples of tokens that aren't \"words\":\n",
    "\n",
    "* Punctuation. For example, the period makes sense as a token, as certain words tend to end sentences (i.e. appear right before a `'.'`), while other words tend to begin sentences (i.e. appear right after a `'.'`).\n",
    "* We have special \"START\" and \"END\" tokens that begin and end every word sequence (in our case, paragraphs of words in a given book). These make sense as tokens, as certain words may tend to begin and end paragraphs. \n",
    "\n",
    "It is useful for the tokens used to represent START and END to be single characters that can never be found in the text of the book you use to create your language model. Thus, you will use two ASCII hidden \"[control characters](https://en.wikipedia.org/wiki/C0_and_C1_control_codes#C0_(ASCII_and_derivatives))\":\n",
    "* For START, you will use the character `'\\x02'`, which refers to the \"beginning of text\".\n",
    "* For END, you will use the character `'\\x03'`, which refers to the \"end of text\".\n",
    "\n",
    "### Checking Your Work for Correctness\n",
    "\n",
    "To build a language model, you will need to perform several steps, from data cleaning to implementing mathematical formulas. **Things will get messy, and it can be difficult to assess the correctness of your work.**\n",
    "\n",
    "- The `grader.check` public notebook tests will be informative, but as per usual, they won't guarantee that your code is correct.\n",
    "- Run your functions on **real books** taken from [Project Gutenberg](https://www.gutenberg.org/) and make sure that their behavior looks correct. After selecting a book ([example](https://www.gutenberg.org/ebooks/68085/)), click the \"Plain Text UTF-8\" link to find the book's text ([example](https://www.gutenberg.org/cache/epub/68085/pg68085.txt))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Navigating the Project\n",
    "\n",
    "Below is an outline of the the project. If there are terms you don't understand in following descriptions, you should review the introduction for an explanation of the terms and ideas described.\n",
    "\n",
    "- [Part 1: Dissecting the Corpus 🪚🫀](#part1)\n",
    "    - [Question 1: Preparing the Corpus](#question1)\n",
    "    - [Question 2: Tokenizing the Corpus](#question2)\n",
    "- [Part 2: Creating Baseline Language Models 📕](#part2)\n",
    "    - [Question 3: Uniform Language Models](#question3)\n",
    "    - [Question 4: Unigram Language Models](#question4)\n",
    "- [Part 3: Creating an N-Gram Language Model 📚](#part3)\n",
    "    - [Question 5.1: Creating N-Grams](#question5a)\n",
    "    - [Question 5.2: Training the N-Gram LM](#question5b)\n",
    "    - [Question 5.3: Computing Probabilities using the N-Gram Model](#question5c)\n",
    "    - [Question 5.4: Sampling from the N-Gram Model](#question5d)\n",
    "    \n",
    "**Disclaimer: You should expect Question 5/Part 3 to take the same amount of time as Parts 1 and 2 combined.** Do not leave it to the last minute just because it looks like there is only \"one question\"! Note also that the entire project is worth **100 points**, of which **50 come from Question 5/Part 3 alone**.\n",
    "\n",
    "Good luck – let's get started! 🎉\n",
    "    \n",
    "---\n",
    "\n",
    "While working on the project, check the Ed post titled \"Project 4 Released!\" for any clarifications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dissecting the Corpus 🪚🫀\n",
    "\n",
    "<a name='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 – Preparing the Corpus\n",
    "\n",
    "<a name='question1'></a>\n",
    "\n",
    "In this question, you will use the `requests` module to download the \"Plain Text UTF-8\" text of a public domain book from [Project Gutenberg](https://www.gutenberg.org/) and prepare it for analysis in later questions. For instance, the book Beowulf's \"Plain Text UTF-8\" URL is [here](https://www.gutenberg.org/ebooks/16328.txt.utf-8), which can be accessed by clicking the \"Plain Text UTF-8\" link [here](https://www.gutenberg.org/ebooks/16328).\n",
    "\n",
    "Complete the implementation of the function `get_book`, which takes in the `url` of a \"Plain Text UTF-8\" book and **returns a string** containing the contents of the book. \n",
    "\n",
    "The returned string should satisfy the following conditions:\n",
    "* The contents of the book consist of **everything** between Project Gutenberg's START and END comments (but not including the START and END comments themselves).\n",
    "* The contents **do** include the title, author, and table of contents.\n",
    "* You should replace any Windows newline characters (`'\\r\\n'`) with standard newline characters (`'\\n'`).\n",
    "* You should check Project Gutenberg's [`robots.txt`](https://gutenberg.org/robots.txt) as well and implement a \"pause\" in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy.\n",
    "\n",
    "***Notes:*** \n",
    "- There is no need to use BeautifulSoup here, so please don't import it.\n",
    "- You are encouraged to find whatever books on the website that interest you to test your code and the language models you develop. The text doesn't even need to be an English-language book.\n",
    "- In the public test example, the START comment is `*** START OF THIS PROJECT GUTENBERG EBOOK BEOWULF ***`. The END comment is formatted similarly. Browse through several book text files before writing your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "beowulf = get_book('https://www.gutenberg.org/ebooks/16328.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 – Tokenizing the Corpus\n",
    "\n",
    "<a name='question2'></a>\n",
    "\n",
    "Now, you need to **tokenize** the text of a book. To do so, complete the implementation of the function `tokenize`, which takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) in the book such that:\n",
    "* The start of every paragraph is represented in the list with the single character `'\\x02'` (standing for START).\n",
    "* The end of every paragraph is represented in the list with the single character `'\\x03'` (standing for STOP).\n",
    "* Tokens include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break, and whitespace (e.g. multiple newlines) between two paragraphs of text do not appear as tokens.\n",
    "* All punctuation marks count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "For example, consider the following excerpt. (The first sentence is at the end of a larger paragraph, and the second sentence is at the start of a longer paragraph.)\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call!!\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```py\n",
    "[...\n",
    "'My', 'phone', \"'\", 's', 'dead', '.', '\\x03', '\\x02', 'I', 'didn', \"'\", 't', 'get', 'your', 'call', '!', '!'\n",
    "...]\n",
    "```\n",
    "\n",
    "***Note:*** \n",
    "- The the list of tokens you return should always start with `'\\x02'` and end with `'\\x03'` (to signify the start and end of the text) regardless of what the text input is.\n",
    "- `tokenize` should run quickly; you should avoid loops. Specifically, `tokenize` should run on the the complete works of Shakespeare (in `data/shakespeare.txt`) in **under 5 seconds** to get full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to prepare the autograder tests. It runs your `tokenize` function on the full body of Shakespeare's work. As a guide, you should expect the first three elements of `shakes` to be `'\\x02'`, `'The'`, and `'Complete'`, and the last three elements of `shakes` to be `'William'`, `'Shakespeare'`, and `'\\x03'`.\n",
    "\n",
    "NOTE: If you are running into issues with opening the `'data/shakespeare.txt'` file, add the argument `encoding='UTF-8'` to the `open()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "import time\n",
    "start = time.time()\n",
    "shakes = tokenize(open('data/shakespeare.txt').read())\n",
    "elapsed = time.time() - start\n",
    "elapsed # Should be (much) under 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part2'></a>\n",
    "\n",
    "## Part 2: Creating Baseline Language Models 📕\n",
    "\n",
    "Now that we're able to tokenize a corpus, it is time to start building language models (LM).\n",
    "\n",
    "In this project, we will build three different language models. They all operate under the premise of assigning probabilities to sentences. Given a sentence – that is, a sequence of tokens $w = w_1\\ldots w_n$ – we want to be able to compute the **probability** that sentence is used: \n",
    "$$P(w) = P(w_1,\\ldots,w_n)$$\n",
    "\n",
    "However, sentences are built from tokens, and the likelihood that a token occurs where it does depends on the tokens before it. This points to using **conditional probability** to compute $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "This is also called the **chain rule** for probabilities.\n",
    "\n",
    "**Example:** Consider the sentence \n",
    "\n",
    "<center><code>'when I drink Coke I smile'</code></center>\n",
    "    \n",
    "The probability that it occurs, according the the chain rule, is\n",
    "\n",
    "$$\n",
    "P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I})\\cdot P(\\text{Coke | when I drink}) \\cdot P(\\text{I | when I drink Coke}) \\cdot P(\\text{smile | when I drink Coke I})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent token follows the tokens that came before. For example, the probability $P(\\text{Coke | when I drink})$ is likely pretty high, as Coke is something that you drink. The probability $P(\\text{pizza | when I drink})$ is likely low, because pizza is not something that you drink.\n",
    "\n",
    "You may wonder how the language model \"knows\" that Coke is something that you drink, but pizza is not. The way that the language model \"learns\" its probabilities is by **looking at examples of existing sentences**, i.e. by being **trained on a corpus**. Throughout Parts 2 and 3, you will look at **different ways of estimating these probabilities**. In each case, you will use a corpus to assign probabilities to different tokens and combinations of tokens, and use those probabilities to generate new sentences.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each language model you build will be a **class** with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the \"training corpus\" on which your model will be trained (i.e. a list of tokens you created in Question 2 with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. **This language model is represented as a `Series`, whose index consists of tokens and whose values are the probabilities that the tokens occur.** (In the case of the N-Gram model in Part 3/Question 5, the model will be represented as a DataFrame instead of a Series – more on this later.)\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a positive integer `M` and generates a string made up of `M` tokens using the language model. **This method generates random sentences!**\n",
    "\n",
    "The description of Question 3 walks through in detail how each of these methods work. However, here's the general workflow:\n",
    "\n",
    "$$\\text{initialize LM with tokenized corpus} \\rightarrow \\text{train LM (i.e. assign probabilities to each token) using corpus} \\rightarrow \\text{used trained LM to compute probabilities of input sentences and generate random sentences}$$\n",
    "\n",
    "In Questions 3, 4, and 5, you will create classes for different language models – uniform, unigram, and N-Gram, respectively. In each class, you will implement each of the above methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 – Uniform Language Models\n",
    "\n",
    "<a name='question3'></a>\n",
    "\n",
    "A uniform language model is one in which each **unique** token is equally likely to appear in any position, unconditional of any other information.\n",
    "\n",
    "Let's put into context what this means by using the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Given a tokenized corpus, you need to build the language model itself in `train`. As mentioned above, language models are stored as Series, where words are the index and probabilities are the values. **In a uniform language model**, the probability assigned to each token is **1 over the total number of unique tokens in the corpus**.\n",
    "\n",
    "The example corpus above has 14 **unique** tokens. This means that we'd have $P(\\text{\\x02}) = \\frac{1}{14}$, $P(\\text{when}) = \\frac{1}{14}$, and so on. Specifically, in this example, **the Series that `train` returns should contain the following values**:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{14}$ |\n",
    "| `'when'` | $\\frac{1}{14}$ |\n",
    "| `'I'` | $\\frac{1}{14}$ |\n",
    "| `'eat'` | $\\frac{1}{14}$ |\n",
    "| `'pizza'` | $\\frac{1}{14}$ |\n",
    "| `','` | $\\frac{1}{14}$ |\n",
    "| `'smile'` | $\\frac{1}{14}$ |\n",
    "| `'but'` | $\\frac{1}{14}$ |\n",
    "| `'drink'` | $\\frac{1}{14}$ |\n",
    "| `'Coke'` | $\\frac{1}{14}$ |\n",
    "| `'my'` | $\\frac{1}{14}$ |\n",
    "| `'stomach'` | $\\frac{1}{14}$ |\n",
    "| `'hurts'` | $\\frac{1}{14}$ |\n",
    "| `'\\x03'` | $\\frac{1}{14}$ |\n",
    "\n",
    "Note that:\n",
    "- **None of the probabilities we computed are conditional – the uniform model does not use conditional probabilities!**\n",
    "- When looking at the Series that `train` returns (i.e. when looking at the `mdl` attribute), the `'\\x02'` and `'\\x03'` characters will show up as blank characters in the index. This is to be expected.\n",
    "- Your Series doesn't need to have the labels `'Token'` or `'Probability'` in it, like the above table does.\n",
    "\n",
    "After training the model, you need to implement two more methods, `probability` and `sample`.\n",
    "\n",
    "`probability` should take in any tuple of tokens and use the probabilities you computed in `train` (that are stored in the `mdl` attribute) to assign a probability to that sequence. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')` (note that this tuple does not need to start with `'\\x02'` or end with `'\\x03'`). To compute the probability of this tuple under our language model, we would multiply the \"trained\" probabilities for each word individually. Here that would give us \n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\left( \\frac{1}{14} \\right)^6$$\n",
    "\n",
    "Note that if the input tuple contains a token that was not in our corpus, `probability` should return 0.\n",
    "\n",
    "Finally, `sample` should take in a positive integer, `M`, and return a sentence made up of `M` randomly sampled tokens, in which the probabilities come from `mdl`. For instance, if `M=5`, then we'd return a sentence containing 5 randomly selected tokens from the table above, such that the probability that each token is selected is $\\frac{1}{14}$. The sampling is done with replacement, so we could end up with the same token multiple times. For instance, we might end up with `'but drink smile drink hurts'`. Note that this sentence doesn't make any grammatical sense (that's okay!) and that tokens are separated by spaces.\n",
    "\n",
    "The starter code contains a class `UniformLM` which represents a uniform language model. Complete the implementation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unif = UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 – Unigram Language Models\n",
    "\n",
    "<a name='question4'></a>\n",
    "\n",
    "A unigram language model is one in which the **probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token**. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus. \n",
    "\n",
    "Let's understand how probabilities are assigned to tokens using our example corpus from before.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 19 total tokens. 3 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{3}{19}$. Here, the Series that `train` returns should contain the following values:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{19}$ |\n",
    "| `'when'` | $\\frac{2}{19}$ |\n",
    "| `'I'` | $\\frac{3}{19}$ |\n",
    "| `'eat'` | $\\frac{1}{19}$ |\n",
    "| `'pizza'` | $\\frac{1}{19}$ |\n",
    "| `','` | $\\frac{3}{19}$ |\n",
    "| `'smile'` | $\\frac{1}{19}$ |\n",
    "| `'but'` | $\\frac{1}{19}$ |\n",
    "| `'drink'` | $\\frac{1}{19}$ |\n",
    "| `'Coke'` | $\\frac{1}{19}$ |\n",
    "| `'my'` | $\\frac{1}{19}$ |\n",
    "| `'stomach'` | $\\frac{1}{19}$ |\n",
    "| `'hurts'` | $\\frac{1}{19}$ |\n",
    "| `'\\x03'` | $\\frac{1}{19}$ |\n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')`. Then,\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\frac{2}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19} \\cdot \\frac{1}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19}$$\n",
    "\n",
    "The `sample` method should now account for the fact that not all tokens are equally likely to be sampled. For instance, `'I'` is much more likely to appear in a randomly generated sentence created by `sample` than `'Coke'` is.\n",
    "\n",
    "The starter code contains a class `UnigramLM` which represents a uniform language model. Complete the implementation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unigram = UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline Language Models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?\n",
    "\n",
    "If you haven't trained your models on the `shakes` corpus, uncomment and run the cells below to do so and generate new random \"sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run – should take less than 10 seconds\n",
    "# shakes_uniform = UniformLM(shakes)\n",
    "# shakes_unigram = UnigramLM(shakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_uniform.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_unigram.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Creating an N-Gram Language Model 📚\n",
    "\n",
    "### Recap\n",
    "\n",
    "First, let's recap what we've done so far. Recall the chain rule for probability, where $w$ is a sentence made up of tokens $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})$$\n",
    "\n",
    "In Questions 3 (Uniform) and 4 (Unigram), your `train` methods computed these probabilities **unconditionally**, meaning that the probability that a token appeared in a sentence did **not depend** on the other tokens around it. That is, you said $P(w_i | w_1, w_2, ..., w_{i - 1}) = P(w_i)$. In Question 3, you let $P(w_i) = \\frac{1}{\\text{# unique tokens in corpus}}$, and in Question 4, you let $P(w_i) = \\frac{\\text{# tokens in corpus equal to $w_i$}}{\\text{# tokens in corpus}}$. Cruciually, each probability was determined by looking at the corpus that the model was trained on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### N-Gram Overview\n",
    "\n",
    "Now we will build an N-Gram language model, in which the probability of a token appearing in a sentence **does depend** on the tokens that come before it. \n",
    "\n",
    "The chain rule above specifies that the probability that a token occurs at in a particular position in a sentence depends on **all** previous tokens in the sentence. However, it is often the case that the likelihood that a token appears in a sentence is influenced more by **nearby** tokens. (Remember, tokens are words, punctuation, or `'\\x02'` / `'\\x03'`).\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Trigram Model\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The trigram model doesn't consider the beginning of the sentence when computing the probability that the sentence ends in `'smile'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, you will need to work with \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, we computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, we must compute a conditional probability for every $N$-token sequence in the corpus. For instance, suppose again that we are training a trigram model. Then, for every 3-token sequence $w_1, w_2, w_3$, we must compute $P(w_3 | w_1, w_2)$. To do so, we use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that we compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "**In the description of Question 5.2 we provide a detailed example of how we might compute such probabilities.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. This code is included for you in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 – Creating N-Grams\n",
    "\n",
    "<a name='question5a'></a>\n",
    "\n",
    "Complete the implementation of the `create_ngrams` method of the `NGramLM` class, which takes in a list of tokens and returns a list of N-Grams, as a tuple. Example behavior is shown below.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGramLM(3, tokens)\n",
    ">>> pizza_model.create_ngrams(tokens)\n",
    "[('\\x02', 'when', 'I'),\n",
    " ('when', 'I', 'eat'),\n",
    " ('I', 'eat', 'pizza'),\n",
    " ('eat', 'pizza', ','),\n",
    " ('pizza', ',', 'I'),\n",
    " (',', 'I', 'smile'),\n",
    " ('I', 'smile', ','),\n",
    " ('smile', ',', 'but'),\n",
    " (',', 'but', 'when'),\n",
    " ('but', 'when', 'I'),\n",
    " ('when', 'I', 'drink'),\n",
    " ('I', 'drink', 'Coke'),\n",
    " ('drink', 'Coke', ','),\n",
    " ('Coke', ',', 'my'),\n",
    " (',', 'my', 'stomach'),\n",
    " ('my', 'stomach', 'hurts'),\n",
    " ('stomach', 'hurts', '\\x03')]\n",
    "```\n",
    "\n",
    "Make sure you understand the above behavior before implementing `create_ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2 – Training the N-Gram LM\n",
    "\n",
    "<a name='question5b'></a>\n",
    "\n",
    "Now, you will compute the probabilities that define N-Gram language model itself. Recall that the N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "which we estimate by  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. To illustrate, consider again the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "Here, `pizza_model.train` must compute $P(\\text{I | \\x02 when})$, $P(\\text{eat | when I})$, $P(\\text{pizza | I eat})$, and so on, until $P(\\text{\\x03 | stomach hurts})$.\n",
    "\n",
    "To compute $P(\\text{eat | when I})$, we must find the number of occurrences of `'when I eat'` in the training corpus, and divide it by the number of occurrences of `'when I'` in the training corpus. `'when I eat'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{eat | when I}) = \\frac{C(\\text{when I eat})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, we will use a DataFrame with three columns, like so:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ngram</th>\n",
    "      <th>n1gram</th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(when, I, drink)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(when, I, eat)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(,, but, when)</td>\n",
    "      <td>(,, but)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(,, I, smile)</td>\n",
    "      <td>(,, I)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(I, smile, ,)</td>\n",
    "      <td>(I, smile)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(,, my, stomach)</td>\n",
    "      <td>(,, my)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(but, when, I)</td>\n",
    "      <td>(but, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(\u0002, when, I)</td>\n",
    "      <td>(\u0002, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(stomach, hurts, \u0003)</td>\n",
    "      <td>(stomach, hurts)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(Coke, ,, my)</td>\n",
    "      <td>(Coke, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>(eat, pizza, ,)</td>\n",
    "      <td>(eat, pizza)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>(I, drink, Coke)</td>\n",
    "      <td>(I, drink)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>(my, stomach, hurts)</td>\n",
    "      <td>(my, stomach)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>(pizza, ,, I)</td>\n",
    "      <td>(pizza, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>(I, eat, pizza)</td>\n",
    "      <td>(I, eat)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>(drink, Coke, ,)</td>\n",
    "      <td>(drink, Coke)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>(smile, ,, but)</td>\n",
    "      <td>(smile, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Here are a couple qualities of this DataFrame that are **important** to note:\n",
    "- Each row in the DataFrame represents each unique trigram in the corpus. As such your model should not include duplicate `ngram`s (though it will likely have duplicate `n1gram`s). \n",
    "- Each row contains the conditional probability for its corresponding trigram. For example the row at position **1** in the above table shows that the probability of the trigram `('when', 'I', 'eat')` conditioned on the bigram `('when', 'I')` is 0.5, as we computed above. \n",
    "- Note that many of the above conditional probabilities are equal to 1 because many trigrams and their corresponding bigrams each appeared only once, and $\\frac{1}{1} = 1$. \n",
    "- Note that `'\\x02'` and `'\\x03'` appear as spaces above, such as in row **7**.\n",
    "\n",
    "\n",
    "After you've understood the above example output, complete the implementation of the `train` method in `NGramLM`. Remember that your model may use any $N$, not just 3 (i.e not just trigrams)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3 – Computing Probabilities using the N-Gram Model\n",
    "\n",
    "<a name='question5c'></a>\n",
    "\n",
    "After we've trained our N-Gram model – that is, after we've computed a DataFrame associating each N-Gram with a conditional probability – we need to compute probabilities for new sentences.\n",
    "\n",
    "To illustrate how this may work, let's look at an example input tuple to `probability`. Assume our model is `pizza_model` from Question 5.2's description; we will not repeat the probability table here.\n",
    "\n",
    "Suppose our input tuple is `('when', 'I', 'eat', 'pizza', ',', 'I', 'smile')`, corresponding to the sentence `'when I eat pizza, I smile'` (remember again that the tuples provided to `probability` don't need to include `'\\x02'` or `'\\x03'`). Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*} &P(\\text{when I eat pizza, I smile}) \\\\ &= P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{eat | when I}) \\cdot P(\\text{pizza | I eat}) \\cdot P(\\text{, | eat pizza}) \\cdot P(\\text{I | pizza,})\\cdot P(\\text{smile | , I}) \\\\ &= \\frac{2}{19} \\cdot 1 \\cdot \\frac{1}{2} \\cdot 1 \\cdot 1 \\cdot 1 \\cdot 1 \\\\ &= \\frac{1}{19} \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- To find the latter five probabilities – $P(\\text{eat | when I}) , P(\\text{pizza | I eat}) , P(\\text{, | eat pizza}) , P(\\text{I | pizza,}),$ and $P(\\text{smile | , I})$, we can use the `mdl` DataFrame that the `train` method computes.\n",
    "- To find $P(\\text{I | when})$, we can't just look at the `mdl` DataFrame, because `('when', 'I')` is not a trigram, it is a bigram. Instead, we look at our model's `prev_mdl` attribute, which itself is another instance of `NGramLM`, corresponding to a bigram model over the same corpus. There, we can find the probability $P(\\text{I | when})$.\n",
    "- To find $P(\\text{when})$, we can't just look at the `mdl` DataFrame, because `'when'` is not a trigram. It is not a bigram either. Instead, we need to look at `prev_mdl`'s `prev_mdl`, which is a `UnigramLM`, to find $P(\\text{when})$.\n",
    "\n",
    "Note that if the input tuple contains an N-Gram that was never seen in the training corpus, the returned probability is 0. Convince yourself why `pizza_model.probability(('when', 'I', 'drink', 'Coke', ',', 'I', 'smile'))` is 0 before proceeding.\n",
    "\n",
    "After you've understood the above example output, complete the implementation of the `probability` method in `NGramLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4 – Sampling from the N-Gram Model\n",
    "\n",
    "<a name='question5d'></a>\n",
    "\n",
    "The last method you implemented in the `UniformLM` and `UnigramLM` classes was `sample`, which gave you a way of generating new sentences. \n",
    "\n",
    "Now, you will implement the `sample` method in the `NGramLM` class. It should take in a positive integer `M` and generate a string of M tokens using the trained language model. It should begin with a starting token `'\\x02'`, then generate a subsequent `M-1` tokens from the probabilities in `self.mdl` while continuing to pick words conditioned on the most recent previous choices, lastly a `'\\x03'` token should be added to the end giving a string of `M` tokens (not counting the inital `'\\x02'` token).\n",
    "\n",
    "Let's illustrate how sampling works using a small concrete example. Suppose our corpus and **trigram** model are defined below:\n",
    "\n",
    "```py\n",
    ">>> short_corpus = 'zebras eat green peas \\n\\n cows eat green grass \\n\\n zebras eat green peppers'\n",
    ">>> short_tokens = tokenize(short_corpus)\n",
    ">>> short_tokens\n",
    "['\\x02', 'zebras', 'eat', 'green', 'peas', '\\x03', '\\x02', 'cows', 'eat', 'green', 'grass', '\\x03', '\\x02', 'zebras', 'eat', 'green', 'peppers', '\\x03']\n",
    ">>> grass_model = NGramLM(3, short_tokens)\n",
    "```\n",
    "\n",
    "Suppose we are told to execute `grass_model.sample(5)`. Here's how we'd proceed:\n",
    "\n",
    "0. The first character in the output is `'\\x02'`, as specified above. **We won't count this starting `'\\x02'` in the length of our output string**, so we still need to find 5 more tokens.\n",
    "1. The next character needs to be either `'zebras'` or `'cows'`, since `('\\x02', 'zebras')` and `('\\x02', 'cows')` are the only **bigrams** in `short_tokens` that start with an `'\\x02'`. $P(\\text{zebras | \\x02})$ is $\\frac{2}{3}$ and $P(\\text{cows | \\x02})$ is $\\frac{1}{3}$, so we select either `'zebras'` or `'cows'` for our next token according to these probabilities. For the sake of example, suppose we select `'cows'`. 4 more tokens to go.\n",
    "2. Now, we must look for **trigrams** that start with the bigram `('\\x02', 'cows')`. There is just one, `('\\x02', 'cows', 'eat')`, so our next token must be `'eat'`. 3 more tokens to go.\n",
    "3. Now, we must look for **trigrams** that start with the bigram `('cows', 'eat')`. Again, there is just one, `('cows', 'eat', 'green')`, so our next token must be `'green'`. 2 more tokens to go.\n",
    "4. Now, we must look for **trigrams** that start with the bigram `('eat', 'green')`. There are three options – `('eat', 'green', 'peas')`, `('eat', 'green', 'grass')`, and `('eat', 'green', 'peppers')`. Since $P(\\text{peas | eat green}) = P(\\text{grass | eat green}) = P(\\text{peppers | eat green}) = \\frac{1}{3}$, we pick either `'peas'`, `'grass'`, or `'peppers'` uniformly at random. For the sake of example, suppose we select `'peppers'`. 1 more token to go.\n",
    "5. We must end the output string now with `'\\x03'`, putting us at `'\\x02'` plus 5 tokens, which is the number of tokens we were told to sample. Note that `'\\x03'` **does** count towards the number of tokens we were asked to sample. \n",
    "\n",
    "Our result is `'\\x02 cows eat green peppers \\x03'`, this is 5 tokens excluding the initial `'\\x02'`. **Note that in our training corpus we never encountered an instance of cows 🐄 eating green peppers 🫑, but we were able to generate a coherent sentence in which they did – pretty cool!**\n",
    "\n",
    "\n",
    "Some additional guidance:\n",
    "- Only the initial `'\\x02'` token is not counted for the number of tokens generated, every subsequent `'\\x02'` or `'\\x03'` token you generate will count towards that total (including the mandatory final `'\\x03'`).\n",
    "- If you run into a situation where there are no N-Grams that match the most recent (N-1)-Gram, you should add `'\\x03'` (STOP) token as the next token in your output sentence. There is a chance that your sampled sentence ends in many `'\\x03'`s, and that's fine.\n",
    "- Helper functions and recursion will be very helpful.\n",
    "- You will not be tested for a case where `M` (the number of tokens you should generate) is less than `N` from your ngrams.\n",
    "\n",
    "After you've understood the above example, complete the implementation of the `sample` method in `NGramLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "bigrams = NGramLM(2, tokens)\n",
    "out = bigrams.create_ngrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = \"\\x02 Humpty Dumpty sat on a wall , Humpty Dumpty had a great fall . \\x03 \\x02 All the king ' s horses and all the king ' s men couldn ' t put Humpty together again . \\x03\".split()\n",
    "tokens = tuple(tokens)\n",
    "ngram = NGramLM(2, tokens)\n",
    "out_5a1 = ngram.create_ngrams(tokens)\n",
    "out_5b1 = ngram.mdl\n",
    "out_5c1 = ngram\n",
    "out_5d1 = ngram.sample(500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've built an N-Gram language model, let's use it to actually generate sentences!\n",
    "\n",
    "Uncomment and run the cell below to define a bigram model using the `shakes` corpus and to generate a sentence of length 50 using the model. **The cell should run in under 30 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run\n",
    "# shakes_bigram = NGramLM(2, shakes)\n",
    "# shakes_bigram.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden tests in Part 3/Question 5 will test your `NGramLM` implementation on corpuses that are much longer than the public tests. One of the corpuses we will test your implementation on is in `data/homertokens.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_tokens = tuple(open('data/homertokens.txt').read().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, it's a good idea to make sure that you can instantiate an `NGramLM` object using `homer_tokens` and that all methods (`probability`, `sample`) run in under ~20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGramLM(5, homer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're satisfied with your `NGramLM` implementation, do a little bit of reflecting. How might you improve the `NGramLM` model? One major deficit is that it assigns a probability of 0 to sentences that contain N-Grams that weren't seen in the corpus; how might you address this? _Hint: You encountered a similar issue when learning Naïve Bayes in DSC 40A! 😊_\n",
    "\n",
    "You don't need to actually make any improvements to `NGramLM`, these are just points for you to think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you've finished Project 4! 🎉\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `project.py` – this notebook should not be uploaded because there are no manually-graded questions in this project.\n",
    "\n",
    "To verify that all of your work is indeed in `project.py`, and that you didn't accidentally implement a function in this notebook and not in `project.py`, we've included another notebook in the project folder, called `project-validation.ipynb`. `project-validation.ipynb` is a version of this notebook with only the `grader.check` cells and the code needed to set up the tests. \n",
    "\n",
    "### **Go to `project-validation.ipynb`, and go to Kernel > Restart & Run All.** This will check if all `grader.check` test cases pass using just the code in `project.py`.\n",
    "\n",
    "Once you're able to pass all test cases in `project-validation.ipynb`, including the call to `grader.check_all()` at the very bottom, then you're ready to submit your `project.py` (and only your `project.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(beowulf, str)\nTrue",
         "failure_message": "check type to make sure the hidden tests will work",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> beowulf[:20] == '\\n\\n\\n\\nBEOWULF\\nAN ANGLO'\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> elapsed <= 5\nTrue",
         "failure_message": "shakespeare fast enough",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[:3] == ['\\x02', 'The', 'Complete'] and shakes[-3:] == ['William', 'Shakespeare', '\\x03']\nTrue",
         "failure_message": "check beginning and ending",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (unif.mdl == 0.25).all()\nTrue",
         "failure_message": "only one probability",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> unif.mdl.shape[0] == 4\nTrue",
         "failure_message": "number of tokens in mdl",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(unif.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unif.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('one', 'two')) == 0.0625\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unif.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unif.sample(1000).split()) == 1000\nTrue",
         "failure_message": "length of sample",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> unigram.mdl.nunique() == 3\nTrue",
         "failure_message": "test mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unigram.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unigram.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unigram.sample(1000).split()) == 1000\nTrue",
         "failure_message": "sample length",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out[0], tuple)\nTrue",
         "failure_message": "test create_ngrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out[0] == ('\\x02', 'one') and out[2] == ('two', 'three')\nTrue",
         "failure_message": "test create_ngrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(bigrams.mdl.columns) == set('ngram n1gram prob'.split())\nTrue",
         "failure_message": "test train",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bigrams.mdl.shape == (6, 3) and bigrams.mdl['prob'].min() == 0.5\nTrue",
         "failure_message": "test train",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> tokens = tuple('\\x02 one two one three one two \\x03'.split())\n>>> bigrams2 = NGramLM(2, tokens)\n>>> p = bigrams2.probability('two one three'.split())\n>>> np.isclose(p, (1/4) * (1/2) * (1/3), 0.01)\nTrue",
         "failure_message": "test probability",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> samp = bigrams.sample(3)\n>>> len(samp.split()) == 4  # don't count the initial START token.\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> samp = bigrams.sample(3)\n>>> samp[:2] == '\\x02 ' and  set(samp.split()) <= {'\\x02', '\\x03', 'one', 'two', 'three', 'four'}\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> 36 <= len(out_5a1) <= 40 and all([len(x) == 2 for x in out_5a1]) and out_5a1[0] == ('\\x02', 'Humpty')\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(out_5b1.groupby('n1gram')['prob'].sum(), 1.0, 1.0).all() and (out_5b1['n1gram'].apply(len) == 1).all()\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> (out_5b1['ngram'].apply(len) == 2).all() and \\\n... np.isclose(out_5b1.loc[out_5b1.ngram == ('Humpty', 'Dumpty'), 'prob'].squeeze(), 0.666666, atol=0.1)\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(out_5c1.probability('Humpty Dumpty sat on a great fall'.split()), 0.01282051282051282, atol=0.1)\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> vc = pd.Series(out_5d1.split()).value_counts(normalize=True)\n>>> {'\\'', 'the', 'Humpty'}.intersection(set(vc.index[:6])) != {}\nTrue",
         "failure_message": "most common sampled tokens",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
