{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14cebd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05818497",
   "metadata": {},
   "source": [
    "# Lab 9 ‚Äì Models and Pipelines üîÅ\n",
    "\n",
    "## DSC 80, Fall 2023\n",
    "\n",
    "### Due Date: Monday, December 4th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc4ace",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Welcome to the ninth lab assignment in DSC 80 this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2023-wi).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- **To ensure that all of your work to be submitted is in `lab.py`, we've provided an additional uneditable notebook, called `lab-validation.ipynb`, that contains only the tests and their setup. Make sure you are able to run it top-to-bottom without error before submitting!**\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35797b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from pipeline_testing_util import get_transformers\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e60d5",
   "metadata": {},
   "source": [
    "## Part 1: `sklearn` Pipelines üß†\n",
    "\n",
    "The file `data/toy.csv` contains an example dataset that consists of 4 columns:\n",
    "\n",
    "- `'group'`: a categorical column with 3 categories\n",
    "- `'c1'`: a numeric attribute\n",
    "- `'c2'`: a numeric attribute\n",
    "- `'y'`: the target variable (that you want to predict) \n",
    "```\n",
    "\n",
    "In the following questions, you will build `Pipeline`s that combine feature engineering with a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f55ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'toy.csv')\n",
    "data = pd.read_csv(fp)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13afbee5",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "First, you will train a regression model using only a *log-scaled* `'c2'` variable. Create a `Pipeline` that:\n",
    "1. log-scales `'c2'`, then\n",
    "2. predicts `'y'` using a linear regression model (using your transformed `'c2'`).\n",
    "\n",
    "That is, complete the implementation of the function `simple_pipeline`, which takes in a DataFrame like `data` and returns a **tuple** consisting of \n",
    "- An already-fit `Pipeline`, and\n",
    "- An array containing the predictions your model makes on `data` (after being trained on `data`).\n",
    "\n",
    "***Note:*** By \"log\", we're referring to the natural logarithm. In order to log-scale `'c2'`, you'll need to use a `FunctionTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d462d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11bee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "q1_fp = os.path.join('data', 'toy.csv')\n",
    "q1_data = pd.read_csv(q1_fp)\n",
    "q1_pl, q1_preds = simple_pipeline(q1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc14b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40653122",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Now, you will engineer features from the other columns and use them to train a regression model.  Create a `Pipeline` that:\n",
    "1. uses `'c1'` as is,\n",
    "1. log-scales `'c2'`,\n",
    "1. one-hot encodes `'group'`, and\n",
    "1. predicts `'y'` using a linear regression model built on the three variables above. (Note that your model will have more than three \"features\", because one-hot encoding `'group'` will create multiple columns. Don't drop any of them.)\n",
    "\n",
    "That is, complete the implementation of the function `multi_type_pipeline`, which takes in a DataFrame like `data` and returns a **tuple** consisting of\n",
    "- An already-fit `Pipeline`, and\n",
    "- An array containing the predictions your model makes on `data` (after being trained on `data`).\n",
    "\n",
    "***Hint:*** Use `ColumnTransformer`, as we did in [Lecture 22](https://dsc80.com/resources/lectures/lec22/lec22.html#Planning-our-first-ColumnTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16bd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c3fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "q2_fp = os.path.join('data', 'toy.csv')\n",
    "q2_data = pd.read_csv(q2_fp)\n",
    "q2_pl, q2_preds = multi_type_pipeline(q2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86676ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4950ae",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "It seems like `'c1'` and `'c2'` have strong associations with the values of `'group'` (to see this, run the cell below). This suggests that group-wise scaling might make good features. \n",
    "\n",
    "\n",
    "Now, we want to standardize (i.e. z-score) both `'c1'` and `'c2'` **within each `'group'`** (`'A'`, `'B'`, and `'C'`). Unfortunately, there is no built-in transformer in `sklearn` that performs group-wise standardization, so **you will need to create your own transformer!**\n",
    "\n",
    "Your job is to complete the implementation of the `StdScalerByGroup` transformer class, meaning that you need to implement the `fit` and `transform` methods, along with the constructor (`__init__`).\n",
    "- The `StdScalerByGroup` transformer works on an input array/DataFrame `X` whose first column contains groups, and whose remaining columns are quantitative and need to be standardized (within each group).\n",
    "- The `fit` method should determine the mean and standard deviation of each quantitative column within each group in the input data `X` and save them in the instance variable `grps_`. (For instance, one of the quantities you may calculate here is the standard deviation of `'c1'`, but only for the rows whose `'group'` is `'B'`.)\n",
    "- The `transform` method should take in an input array/DataFrame `X`, standardize each quantitative column separately using the means and standard deviations stored in `grps_`, and return a DataFrame containing the transformed quantitative columns.\n",
    "\n",
    "\n",
    "If you `fit` and `transform` a `StdScalerByGroup` transformer on the `toy` DataFrame (without the `'y'` column), you should get back a DataFrame with two columns, `'c1'` and `'c2'`, with groups stored in the index (if you end up creating a `MultiIndex`, that is fine).\n",
    "\n",
    "\n",
    "***Notes:***\n",
    "1. You may decide on whatever structure you'd like for the `grps_` variable. This question will be graded on the correctness of the output of your transformer. (Check the correctness of your work by checking the output by-hand!)    \n",
    "2. At no point should you loop over the **rows** of `data` (in fact, our solution doesn't use any loops), but you are allowed to use loop in this question.\n",
    "3. The `'group'` column in the public tests is named `'g'` instead of `'group'`. Remember, the first column will **always** contain the groups, even if the first column's name is something other than `'group'`.\n",
    "4. Do not worry about cases where the standard deviation is equal to 0, the function `std` might work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ff4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter plot referenced at the start of Question 3\n",
    "# This is not needed to answer the question, but motivates why we are standardizing\n",
    "px.scatter(data, x='c1', y='y', color='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5371b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97da6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0943bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "# test fit \n",
    "q3_test_fit_cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 2, 2], 'c2': [3, 1, 2, 0]}\n",
    "q3_test_fit_X = pd.DataFrame(q3_test_fit_cols)\n",
    "q3_test_fit_std = StdScalerByGroup().fit(q3_test_fit_X)\n",
    "\n",
    "# test transform\n",
    "q3_test_transform_cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 3, 4], 'c2': [1, 2, 3, 4]}\n",
    "q3_test_transform_X = pd.DataFrame(q3_test_transform_cols)\n",
    "q3_test_transform_std = StdScalerByGroup().fit(q3_test_transform_X)\n",
    "q3_test_transform_out = q3_test_transform_std.transform(q3_test_transform_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6223998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests to work\n",
    "q3_fit_data = pd.read_csv('data/toy.csv')\n",
    "\n",
    "N = 2*10**6\n",
    "a = np.random.choice(['A', 'B'], size=(N,1)).astype('object')\n",
    "b = np.random.multivariate_normal([1, 2], [[1, 0],[0, 100]], size=N)\n",
    "arr = np.hstack([a, b])\n",
    "q3_transform_data = pd.DataFrame(arr)\n",
    "q3_transform_data[1] = q3_transform_data[1].astype(float)\n",
    "q3_transform_data[2] = q3_transform_data[2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78cfd3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4b356",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "`Pipeline`s are supposed to help you easily try different model configurations. Complete the implementation of the function `eval_toy_model`, which returns a hard-coded **list of tuples** consisting of the (RMSE, $R^2$) of three different modeling `Pipeline`s, fit and evaluated on the entire input dataset `data`. The three different `Pipeline`s are:\n",
    "1. The `Pipeline` in Question 1.\n",
    "1. The `Pipeline` in Question 2.\n",
    "1. A `Pipeline` consisting of a linear regression model fit on features generated by applying `StdScalerByGroup` to `'c1'`, log-scaling `'c2'`, and applying `OneHotEncoder` to `'group'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94353648",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da917bc2",
   "metadata": {},
   "source": [
    "## Part 2: Overfitting üòü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b8e39",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "In this question, you will train two different classes of prediction models ‚Äì **decision tree and k-Nearest Neighbor regressors** ‚Äì and explore different ways in which overfitting can appear. You'll use Galton's child heights dataset from the missingness lectures and Lab 5.\n",
    "\n",
    "#### `tree_reg_perf` üå≤\n",
    "\n",
    "A decision tree regressor is trained similar to a decision tree classifier: the splits of the tree are created by minimizing the variance of the (training data) response values in the leaves given by making the split in question. A decision tree regressor predicts the response value of a (new) observation based on the **average target value** of the training observations lying in the same leaf node. \n",
    "\n",
    "One **hyperparameter** of a decision tree regressor that affects model complexity is the **depth** of the tree. Larger depths correspond to more complicated decision trees. We will explore this parameter in this question.\n",
    "\n",
    "Complete the implementation of the function `tree_reg_perf`, which takes in a DataFrame like `galton` and:\n",
    "- Splits the data into training and test sets,\n",
    "- Trains 20 decision trees ‚Äì one for each depth between 1 and 20, and\n",
    "- Computes both the training RMSE and testing RMSE of each tree.\n",
    "\n",
    "Store and return your results in a DataFrame that has two columns, `'train_err'` and `'test_err'`, and an index that corresponds to tree depths (i.e. 1, 2, ..., 20).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `knn_reg_perf` üëâüëà\n",
    "\n",
    "A k-Nearest Neighbors (k-NN) regressor predicts the response value of a (new) observation by computing the average value of the k-closest observations in the training set. The most common distance metric is Euclidean distance, i.e. $L_2$ distance.\n",
    "\n",
    "One **hyperparameter** of a k-NN regressor that affects model complexity is k, **the number of neighbors averaged over**. Larger values of k correspond to more complicated regressors. We will explore this hyperparameter in this question.\n",
    "\n",
    "Complete the implementation of the function `knn_reg_perf`, which takes in a DataFrame like `galton` and:\n",
    "- Splits the data into training and test sets,\n",
    "- Trains 20 k-NN regressors ‚Äì one for each value of k between 1 and 20, and\n",
    "- Computes both the training RMSE and testing RMSE of each regressor.\n",
    "\n",
    "Again, store and return your results in a DataFrame that has two columns, `'train_err'` and `'test_err'`, and an index that corresponds to values of k (i.e. 1, 2, ..., 20).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Some guidelines for both subparts:**\n",
    "\n",
    "- In all cases, we are using all other columns in `galton` to predict `'childHeight'`.\n",
    "- You need to import the necessary classes from `sklearn` **inside** the functions you create. (Unlike before, we haven't imported them for you because we want you to figure out what to import!)\n",
    "- If you're unsure how to create training and testing sets, refer to [Lecture 23](https://dsc80.com/resources/lectures/lec23/lec23.html). Use a test set size of 0.25. Within each function, you should only perform a single train-test split ‚Äì that is, you should not be creating training and testing sets within a `for`-loop (though you will need to use a `for`-loop for something else).\n",
    "    - For the purposes of this question, do not use any cross-validation.\n",
    "- Don't write the formula for RMSE four times ‚Äì define a helper function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `galton` to test your work.\n",
    "galton = pd.read_csv('data/galton.csv')\n",
    "galton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af56087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806fa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "galton_test = pd.read_csv('data/galton.csv')\n",
    "out_tree_test = tree_reg_perf(galton_test)\n",
    "out_knn_test = knn_reg_perf(galton_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58674469",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e86952",
   "metadata": {},
   "source": [
    "After you've implemented both functions, run the cells below to plot training and testing error for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67035",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9) # For reproducibility\n",
    "\n",
    "tree = tree_reg_perf(galton)\n",
    "knn = knn_reg_perf(galton)\n",
    "hyp = np.arange(1, 21)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Error vs. Tree Depth<br>for Decision Tree Regressor',\n",
    "                                                    'Error vs. k (# Neighbors)<br>for k-NN Regressor'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hyp, y=tree.iloc[:, 0], line=dict(color='blue'), name='Training Error'),\n",
    "    row=1, col=1).add_trace(go.Scatter(x=hyp, y=tree.iloc[:, 1], line=dict(color='red'), name='Testing Error'), \n",
    "                            row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hyp, y=knn.iloc[:, 0], line=dict(color='blue'), name='Training Error', showlegend=False),\n",
    "    row=1, col=2).add_trace(go.Scatter(x=hyp, y=knn.iloc[:, 1], line=dict(color='red'),  name='Testing Error',\n",
    "                                       showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=450, width=975)\n",
    "fig.update_xaxes(title_text='Tree Depth', row=1, col=1, tickvals=np.arange(1,21,2))\n",
    "fig.update_xaxes(title_text='k (# Neighbors)', row=1, col=2, tickvals=np.arange(1,21,2))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705287e",
   "metadata": {},
   "source": [
    "If your training and evaluation routines are correct, you should notice a few things:\n",
    "- In both models, testing error initially decreases, and then (perhaps slowly) increases.\n",
    "- With the decision tree, training error **decreases** as depth increases.\n",
    "- With the k-NN regressor, training error **increases** as k (the number of neighbors looked at) increases.\n",
    "\n",
    "You should think about **why** you observe each of the above phenomena. In particular, the last point may seem confusing ‚Äì one would think that because larger values of k correspond to more complicated models (because the regressor is looking at more information to make a prediction), larger values of k should have lower training errors. But the nature of k-NN regressors is quite different than, say, decision tree regressors or linear regression models.\n",
    "\n",
    "Lastly, in both cases, identify the ideal **hyperparameter** choice based on the graphs of testing error. You don't have to write the answer anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36a060",
   "metadata": {},
   "source": [
    "## Part 3: Predicting Survival on the Titanic üõ≥üßä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c28b3",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Predicting whether or not passengers on the Titanic survived is a common first assignment when learning about classification ‚Äì now it's your turn!\n",
    "\n",
    "Complete the implementation of the function `titanic_model`, which takes in a DataFrame `titanic` containing **training data only** and returns a `Pipeline` object fit to the training data. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "You have freedom to build your own model. That is, **you can use any classification algorithm**, but your model should satisfy the following requirements:\n",
    "\n",
    "- The model is built on the (binary) response column `'Survived'`.\n",
    "* The model uses features derived from **all other columns in `titanic`**. Below, we specify which columns to \"engineer\"; you may \"engineer\" features using other columns, but be sure to include every column in your model (even if you choose to leave some columns as-is).\n",
    "\n",
    "* Required feature engineering:\n",
    "    * Derive a feature from the \"title\" in the `'Name'` field (e.g. \"Mr\", \"Miss\", \"Mrs\" ‚Äì the names themselves should not be used as a feature; think about why).\n",
    "    * Derive a feature that standardizes passengers' ages among their `'Pclass'` (use Question 3!).\n",
    "    \n",
    "#### Evaluation\n",
    "    \n",
    "Your model must achieve an accuracy of 0.78 on both the training set and the test set. Note that while you have access to the test set, it is still encouraged to perform your own model validation.\n",
    "\n",
    "**Extra credit: If your model can consistently earn an accuracy of above 0.83 on the test set, you can earn 5 points of extra credit on the lab!** That is, you can earn up to 60 points on the lab, even though it is only graded out of 55 points (meaning you _can_ have a score above 100% on the lab).\n",
    "\n",
    "Some guidance:\n",
    "\n",
    "- `Pipeline` objects can have other `Pipeline` objects within them. While this isn't a requirement, you may find this useful to break down your model into smaller, more manageable steps.\n",
    "\n",
    "- You can make make `Pipeline` of just transformers.\n",
    "\n",
    "- Your submitted `titanic_model` function should have the model's hyperparameters (e.g. tree depth) hard-coded in it. That is, the `Pipeline` object doesn't have to include the hyperparameter selection process.\n",
    "\n",
    "- You will find [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) useful. If you want your transformer to output a categorical feature, you will need to select `validate=False`.\n",
    "\n",
    "- When using [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer), you may find the `remainder` keyword helpful.\n",
    "\n",
    "- If you are set out to get those extra 5 points, consider building some meaningful features before fine-tuning the hyperparameters of your model. Do an EDA on the dataset ‚Äì what kinds of people are more prone to survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment using `titanic` below ‚Äì remember, this is only your training data\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09287ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = titanic.drop(columns = 'Survived')\n",
    "y = titanic['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "pre = titanic_model(titanic)\n",
    "(pre.predict(X_test)==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378de75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ea868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "q6_data_test = pd.read_csv('data/titanic.csv')\n",
    "pl_test = titanic_model(q6_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7451373",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07137fe",
   "metadata": {},
   "source": [
    "There is **a ton** of material out there on analyzing data from the Titanic. After you build your model, look online for other examples (e.g. [on Kaggle](https://www.kaggle.com/c/titanic)) and think about how you could improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2de3f",
   "metadata": {},
   "source": [
    "## Congratulations! You've finished _the final lab of the quarter_! üéâü•≥\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To verify that all of your work is indeed in `lab.py`, and that you didn't accidentally implement a function in this notebook and not in `lab.py`, we've included another notebook in the lab folder, called `lab-validation.ipynb`. `lab-validation.ipynb` is a version of this notebook with only the `grader.check` cells and the code needed to set up the tests. \n",
    "\n",
    "### **Go to `lab-validation.ipynb`, and go to Kernel > Restart & Run All.** This will check if all `grader.check` test cases pass using just the code in `lab.py`.\n",
    "\n",
    "Once you're able to pass all test cases in `lab-validation.ipynb`, including the call to `grader.check_all()` at the very bottom, then you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82304eb5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcd00e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q1_pl, Pipeline)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q1_pl.steps[-1][1], LinearRegression)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q1_pl.steps[0][1], FunctionTransformer)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q1_preds.shape[0] == q1_data.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q2_pl, Pipeline)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q2_pl.steps[-1][1], LinearRegression)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q2_pl.steps[0][1], ColumnTransformer)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q2_data.shape[0] == q2_preds.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3_test_fit_std.grps_ is not None\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q3_test_transform_out.shape == (4, 2)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(q3_test_transform_out.abs(), 0.707107, atol=0.001).all().all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = eval_toy_model()\n>>> len(out) == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> out = eval_toy_model()\n>>> np.all([len(t) == 2 for t in out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (out_tree_test.columns.tolist() == ['train_err', 'test_err']) == True\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> (out_tree_test['train_err'].iloc[-1] < out_tree_test['test_err'].iloc[-1]) == True\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> (out_knn_test.columns.tolist() == ['train_err', 'test_err']) == True\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(pl_test, Pipeline)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(pl_test.steps[-1][-1], BaseEstimator)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> preds_test = pl_test.predict(q6_data_test.drop('Survived', axis=1))\n>>> ((preds_test == 0) | (preds_test == 1)).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
