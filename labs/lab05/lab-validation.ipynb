{"cells": [{"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["\n# Lab 5 Validation Notebook\n\nYou cannot write any code in this notebook \u2013 to actually work on the lab, open `lab.py` and `lab.ipynb`.\n\nThe only purpose of this notebook is to give you a blank copy of `lab.ipynb`,\nso that you can go to **Kernel > Restart & Run All** and ensure that all public `grader.check` cells pass using just the code in your `lab.py`.\n\n**Before submitting Lab 5, make sure that the call to `grader.check_all()` at the bottom of this notebook shows that all test cases passed!** \nIf it doesn't, there's likely a function in `lab.ipynb` that is not implemented correctly in `lab.py`, or it could be that a function in `lab.py` depends on an object (e.g. a DataFrame) that is not an argument to that function.\n    "]}, {"cell_type": "code", "execution_count": null, "id": "6507641e", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook(\"lab.ipynb\")"]}, {"cell_type": "code", "execution_count": 1, "id": "63496bbb", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": 2, "id": "5474a05d", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["from lab import *"]}, {"cell_type": "code", "execution_count": 3, "id": "90bdc927", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["import os\n", "import pandas as pd\n", "import numpy as np\n", "from scipy import stats\n", "\n", "import plotly.express as px\n", "pd.options.plotting.backend = 'plotly'\n", "\n", "import requests\n", "import bs4"]}, {"cell_type": "code", "execution_count": 6, "id": "53fabe30", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell -- it is needed for the tests to work\n", "out_q1 = after_purchase()"]}, {"cell_type": "code", "execution_count": null, "id": "4e1716e4", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q1\")"]}, {"cell_type": "code", "execution_count": 20, "id": "829f67d4", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell -- it is needed for the tests to work\n", "out_q2 = multiple_choice()"]}, {"cell_type": "code", "execution_count": null, "id": "bae3a365", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q2\")"]}, {"cell_type": "code", "execution_count": 32, "id": "ecf451f4", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# Run your permutation tests in the Jupyter Notebook;\n", "# put your final results in lab.py\n", "payments_fp = os.path.join('data', 'payment.csv')\n", "payments = pd.read_csv(payments_fp)\n", "payments.head()"]}, {"cell_type": "code", "execution_count": 33, "id": "842f1220", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "first_pval, first_result = first_round()\n", "second_pval, second_result, second_result1 = second_round()"]}, {"cell_type": "code", "execution_count": null, "id": "febedc8c", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q3\")"]}, {"cell_type": "code", "execution_count": 49, "id": "bfb4c49e", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "heights_fp = os.path.join('data', 'missing_heights.csv')\n", "heights = pd.read_csv(heights_fp)\n", "out_q4 = verify_child(heights.copy())"]}, {"cell_type": "code", "execution_count": null, "id": "e17fbbf8", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q4\")"]}, {"cell_type": "code", "execution_count": 57, "id": "a7ed052d", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["out_q4.to_frame().rename(columns={0: 'p-value'}).plot(kind='barh', width=800, height=400)"]}, {"cell_type": "code", "execution_count": 59, "id": "a664030a", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "heights_fp = os.path.join('data', 'missing_heights.csv')\n", "new_heights = pd.read_csv(heights_fp)[['father', 'child_50']]\n", "new_heights = new_heights.rename(columns={'child_50': 'child'})\n", "out_q5 = cond_single_imputation(new_heights.copy())"]}, {"cell_type": "code", "execution_count": 60, "id": "13abb9c4", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "heights_q5 = pd.read_csv('data/missing_heights.csv')\n", "heights_q5['child'] = heights_q5['child_50']\n", "inp_q5 = heights_q5\n", "out_q5 = cond_single_imputation(inp_q5)\n", "df_q5 = inp_q5.copy()\n", "df_q5['imputed'] = out_q5\n", "gp1_q5 = df_q5.groupby('father')['imputed'].mean()\n", "gp2_q5 = df_q5.groupby('father')['child'].mean()\n", "m_q5 = (pd.concat([gp1_q5, gp2_q5], axis=1)\n", "     .dropna().diff(axis=1).abs().iloc[:, -1])"]}, {"cell_type": "code", "execution_count": null, "id": "7d0262d0", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q5\")"]}, {"cell_type": "code", "execution_count": 68, "id": "50448f02", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "heights_fp = os.path.join('data', 'missing_heights.csv')\n", "heights = pd.read_csv(heights_fp)\n", "child = heights['child_50']\n", "quantitative_distribution_out_q6 = quantitative_distribution(child.copy(), 100)\n", "impute_height_quant_out_q6 = impute_height_quant(child.copy())"]}, {"cell_type": "code", "execution_count": null, "id": "e1da5d49", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q6\")"]}, {"cell_type": "code", "execution_count": 78, "id": "caf0515b", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "mc_answers, websites = answers()"]}, {"cell_type": "code", "execution_count": 79, "id": "c65f5f33", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["# don't change this cell, but do run it -- it is needed for the tests to work\n", "from urllib.parse import urlparse\n", "import urllib.robotparser\n", "\n", "# This code checks the robots.txt file\n", "def canFetch(url):\n", "    if url[:4] != 'http':\n", "        url = 'https://' + url\n", "    parsed_uri = urlparse(url)\n", "\n", "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n", "\n", "    rp = urllib.robotparser.RobotFileParser()\n", "    rp.set_url(domain + \"/robots.txt\")\n", "    try:\n", "        rp.read()\n", "        canFetchBool = rp.can_fetch(\"*\", url)\n", "    except:\n", "        canFetchBool = None\n", "    \n", "    return canFetchBool\n", "\n", "ans_q7, websites_q7 = answers()\n", "canfetch_q7 = []\n", "for site in websites_q7:\n", "    try:\n", "        canfetch_q7.append(canFetch(site))\n", "    except:\n", "        canfetch_q7.append(None)"]}, {"cell_type": "code", "execution_count": null, "id": "232e252e", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check(\"q7\")"]}, {"cell_type": "code", "execution_count": null, "id": "add43569", "metadata": {"editable": false, "deletable": false}, "outputs": [], "source": ["grader.check_all()"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["\nIf you were able to go to **Kernel > Restart & Run All** and see all test cases pass above, and you've thoroughly tested your code yourself, you're ready to submit `lab.py` to Gradescope!\n    "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.6"}, "otter": {"tests": {"q1": {"name": "q1", "points": null, "suites": [{"cases": [{"code": ">>> len(out_q1) == 5\nTrue", "failure_message": "output length should be 5", "hidden": false, "locked": false, "points": 1}, {"code": ">>> set(out_q1) <= set(['MD', 'MCAR', 'MAR', 'NMAR'])\nTrue", "failure_message": "output contains answers other than the specified options", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[0] == 'NMAR'\nTrue", "failure_message": "sub-question 1: reviewers are more likely to review when dissatified", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[0] in ['NMAR', 'MCAR']\nTrue", "failure_message": "sub-question 1: partial; one column must be NMAR or MCAR", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[1] == 'MD'\nTrue", "failure_message": "sub-question 2: not the optimal answer", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[1] in ['MD', 'MAR']\nTrue", "failure_message": "sub-question 2: partial", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> out_q1[2] == 'MAR'\nTrue", "failure_message": "sub-question 3: not the optimal answer", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[2] in ['NMAR', 'MAR']\nTrue", "failure_message": "sub-question 3: partial", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> out_q1[3] in ['NMAR', 'MAR']\nTrue", "failure_message": "sub-question 4: not the optimal answer, how does having a serial number affect missingness?", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[3] in ['NMAR', 'MAR', 'MCAR']\nTrue", "failure_message": "sub-question 4: partial", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> out_q1[4] == 'MAR'\nTrue", "failure_message": "sub-question 5: not the optimal answer", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q1[4] in ['MAR', 'NMAR', 'MCAR']\nTrue", "failure_message": "sub-question 5: partial", "hidden": false, "locked": false, "points": 0.5}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q2": {"name": "q2", "points": null, "suites": [{"cases": [{"code": ">>> len(out_q2) == 5\nTrue", "failure_message": "output length should be 5", "hidden": false, "locked": false, "points": 1}, {"code": ">>> set(out_q2) <= set(['MD', 'MCAR', 'MAR', 'NMAR'])\nTrue", "failure_message": "output contains answers other than the specified options", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[0] == 'MAR'\nTrue", "failure_message": "sub-question 1", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[0] in ['MCAR', 'MAR', 'NMAR']\nTrue", "failure_message": "sub-question 1: partial", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[1] in ['MAR', 'NMAR']\nTrue", "failure_message": "sub-question 2", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[2] in ['MD', 'MAR']\nTrue", "failure_message": "sub-question 3", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[3] == 'NMAR'\nTrue", "failure_message": "sub-question 4", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[3] in ['NMAR', 'MCAR']\nTrue", "failure_message": "sub-question 4: partial", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[4] == 'MCAR'\nTrue", "failure_message": "sub-question 5", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q2[4] in ['MAR', 'NMAR', 'MCAR']\nTrue", "failure_message": "sub-question 5: partial", "hidden": false, "locked": false, "points": 1}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q3": {"name": "q3", "points": null, "suites": [{"cases": [{"code": ">>> isinstance(first_round(), list) and isinstance(second_round(), list)\nTrue", "hidden": false, "locked": false, "points": 0.25}, {"code": ">>> len(first_round()) == 2 and len(second_round()) == 3\nTrue", "hidden": false, "locked": false, "points": 0.25}, {"code": ">>> first_pval < 1\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> first_result in ['NR', 'R']\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> second_pval < 1\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> second_result in ['NR', 'R']\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> second_result1 in ['ND', 'D']\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> np.isclose(first_pval, 0.16, atol=0.4)\nTrue", "failure_message": "first_round p-value approximate", "hidden": false, "locked": false, "points": 3}, {"code": ">>> first_result == 'NR'\nTrue", "failure_message": "first_round result -- reject / do not reject", "hidden": false, "locked": false, "points": 2}, {"code": ">>> second_result1 == 'D'\nTrue", "failure_message": "part 3 multiple choice", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(second_pval, 0.02, atol=0.2)\nTrue", "failure_message": "second_round p-value approximate", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(second_pval, 0.02, atol=0.1)\nTrue", "failure_message": "second_round p-value approximate", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(second_pval, 0.02, atol=0.02)\nTrue", "failure_message": "second_round p-value", "hidden": false, "locked": false, "points": 1}, {"code": ">>> second_result == 'R'\nTrue", "failure_message": "second_round result -- reject / do not reject", "hidden": false, "locked": false, "points": 1}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q4": {"name": "q4", "points": null, "suites": [{"cases": [{"code": ">>> out_q4['child_50'] < out_q4['child_95']\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> out_q4['child_5'] > out_q4['child_50']\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> out_q4['child_95'] > out_q4['child_5']\nTrue", "failure_message": "95 > 5", "hidden": false, "locked": false, "points": 1}, {"code": ">>> out_q4['child_75'] > out_q4['child_25']\nTrue", "failure_message": "75 > 25", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(out_q4['child_50'], 0, atol=0.02)\nTrue", "failure_message": "pvalue of child_50", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(out_q4['child_25'], 0.06, atol=0.2)\nTrue", "failure_message": "pvalue of child_25", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(out_q4['child_95'], 0.8, atol=0.2)\nTrue", "failure_message": "pvalue of child_95", "hidden": false, "locked": false, "points": 1}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q5": {"name": "q5", "points": null, "suites": [{"cases": [{"code": ">>> out_q5.isna().sum() == 0\nTrue", "hidden": false, "locked": false, "points": 1}, {"code": ">>> (new_heights['child'].std() - out_q5.std()) > 0.5\nTrue", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(out_q5.mean(), inp_q5['child'].mean(),atol=0.2)\nTrue", "failure_message": "means are close", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.all(np.isclose(m_q5, 0, atol=2))\nTrue", "failure_message": "group-wise means are close: very approx", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(m_q5.median(), 0.314, atol=0.1)\nTrue", "failure_message": "group-wise means are close: approx", "hidden": false, "locked": false, "points": 1}, {"code": ">>> np.isclose(m_q5.median(), 0.31439, atol=0.01)\nTrue", "failure_message": "group-wise means are close", "hidden": false, "locked": false, "points": 1}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q6": {"name": "q6", "points": null, "suites": [{"cases": [{"code": ">>> quantitative_distribution_out_q6.min() >= 56\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> quantitative_distribution_out_q6.max() <= 79\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> np.isclose(quantitative_distribution_out_q6.mean(), child.mean(), atol=1)\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> np.isclose(quantitative_distribution_out_q6.std(), 3.5, atol=0.65)\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> impute_height_quant_out_q6.isna().sum() == 0\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> np.isclose(impute_height_quant_out_q6.mean(), child.mean(), atol=0.5)\nTrue", "hidden": false, "locked": false, "points": 0.5}, {"code": ">>> (set(quantitative_distribution_out_q6) - set(child.dropna())) != {}\nTrue", "failure_message": "new values", "hidden": false, "locked": false, "points": 1}, {"code": ">>> (set(impute_height_quant_out_q6) - set(child.dropna())) != {}\nTrue", "failure_message": "new values", "hidden": false, "locked": false, "points": 1}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}, "q7": {"name": "q7", "points": null, "suites": [{"cases": [{"code": ">>> len(mc_answers) == 4\nTrue", "hidden": false, "locked": false, "points": 1}, {"code": ">>> len(websites) >= 2\nTrue", "hidden": false, "locked": false, "points": 1}, {"code": ">>> ans_q7[0] == 1\nTrue", "failure_message": "multiple choice: 1", "hidden": false, "locked": false, "points": 1}, {"code": ">>> ans_q7[1] == 2\nTrue", "failure_message": "multiple choice: 2", "hidden": false, "locked": false, "points": 1}, {"code": ">>> ans_q7[2] == 2\nTrue", "failure_message": "multiple choice: 3", "hidden": false, "locked": false, "points": 1}, {"code": ">>> ans_q7[3] == 1\nTrue", "failure_message": "multiple choice: 4", "hidden": false, "locked": false, "points": 1}, {"code": ">>> ('waymo.com' not in ''.join(websites)) and ('linkedin.com' not in ''.join(websites))\nTrue", "failure_message": "no waymo or linkedin!", "hidden": false, "locked": false, "points": 1}, {"code": ">>> True in canfetch_q7\nTrue", "failure_message": "at least one that can scrape", "hidden": false, "locked": false, "points": 2}, {"code": ">>> False in canfetch_q7\nTrue", "failure_message": "at least one that can not scrape", "hidden": false, "locked": false, "points": 2}], "scored": true, "setup": "", "teardown": "", "type": "doctest"}]}}}}, "nbformat": 4, "nbformat_minor": 5}