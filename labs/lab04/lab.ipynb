{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 ‚Äì Hypothesis and Permutation Testing\n",
    "\n",
    "## DSC 80, Spring 2023\n",
    "\n",
    "### Due Date: Monday, October 30th at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Welcome to the fourth lab assignment in DSC 80 this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2023-fa).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- **To ensure that all of your work to be submitted is in `lab.py`, we've provided an additional uneditable notebook, called `lab-validation.ipynb`, that contains only the tests and their setup. Make sure you are able to run it top-to-bottom without error before submitting!**\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Time Series Data\n",
    "\n",
    "**Note: You should not use `for`-loops at all in this part!**\n",
    "\n",
    "Imagine that you own an online store and you'd like to monitor the visits to your site. You've collected information about different login dates and times for different users and stored it in `data/login_table.csv`. Some users are unique, while some visited your store multiple times.\n",
    "\n",
    "Answer the questions below to better understand the login patterns of your users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äì Prime Time ‚è∞\n",
    "\n",
    "Complete the implementation of the function `prime_time_logins`, which takes in a DataFrame like `login` and outputs a DataFrame indexed by `'Login Id'`, counting the number of prime-time logins for each user ‚Äì that is, the number of logins that were between 4 PM(inclusive) and 8 PM (exclusive) for each user. The DataFrame should have just one column, named `'Time'`.\n",
    "\n",
    "For example, if a user logs in at 5 PM on Day 1, at 1 PM on Day 2, at 6 PM on Day 2, and at 7 PM on Day 2, then their total number of prime-time logins is 3. Note that the values in your returned DataFrame should only include counts, not timestamp objects.\n",
    "\n",
    "***Note:*** You do not need to use Python's `datetime` module ‚Äì instead, use the built-in `pandas` methods for working with times that we introduced in [Lecture 6](https://dsc80.com/resources/lectures/lec06/lec06.html) (though you may need to do a bit more research to fully answer the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data')/'login_table.csv'\n",
    "login = pd.read_csv(fp)\n",
    "q1_result = prime_time_logins(login)\n",
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ‚Äì Return Users üîÅ\n",
    "\n",
    "As a site owner, you would like to find your most enthusiastic users ‚Äì the ones who return to your site most frequently. You've noticed that there are users who have several logins and users who logged in only once. You are interested in finding the number of logins *per day* for each user.\n",
    "\n",
    "Complete the implementation of the function `count_frequency`, which takes in a DataFrame like `login` and outputs a Series containing the number of logins per day for each user. Your Series should have `'Login Id'`s in its index, and the frequencies as its values. The order of users in the index is arbitrary.\n",
    "\n",
    "To do this, you can assume today is  January 31, 2023. The first login date of a user is the first day of their membership on the site, and you may assume they are still a member today. For simplicity, you only need to count full days that a user has been a member till the end of today. For example, if a user's first login was 12 days and 5 hours ago, you can say that they have been a user for 12 days. \n",
    "\n",
    "***Hint:*** Can you write a custom aggregator that allows you to do this with just one `.groupby`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data')/'login_table.csv'\n",
    "login = pd.read_csv(fp)\n",
    "q2_result = count_frequency(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hypothesis Testing\n",
    "\n",
    "In this section, you'll recall the terms and structure of hypothesis testing from DSC 10.\n",
    "\n",
    "The first step is always to define what you're looking at, create your hypotheses, and set a level of significance (i.e. a p-value cutoff). Once you've done that, you can find a p-value.\n",
    "\n",
    "If all of these words are foreign, look at the [Lecture 6](https://dsc80.com/resources/lectures/lec06/lec06.html) notebook and the readings, and don't forget to think about the real-world meaning of these terms!  The following example describes a real-world scenario, which should help keep it easy to interpret.\n",
    "\n",
    "Note that you **can** use `for`-loops to conduct hypothesis and permutation tests in assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 ‚Äì Surf's Up üèÑ\n",
    "\n",
    "In San Diego, students are looking to surf in their free time. There is a pop-up surf store on Library Walk selling wet suits and surf board to students. Last Saturday, this store sold 250 wet suits to UCSD students. After a surf session, 10 students complained that their wet suits had tears in them, letting cold ocean water to rush in their suits. In response to the student dissatisfaction, the store claims that 98% of their wet suits are produced without any manufacturing defects. You think this seems unlikely and decide to investigate.\n",
    "\n",
    "First, select a significance level for your investigation. You don't need to turn this in anywhere. Then, complete the implementation of the following three functions.\n",
    "\n",
    "#### `null_hyp`\n",
    "\n",
    "Complete the implementation of the function `suits_null_hyp`, which has no parameters and returns your answer to the following question **as a list**.\n",
    "\n",
    "What are reasonable choices for the **null hypothesis** for your investigation? Select all that apply.\n",
    "1. The store sells wet suits that are approximately 2% defective.\n",
    "2. The store sells wet suits that are 98% non-defective.\n",
    "3. The store sells wet suits that are less than 98% non-defective.\n",
    "4. The store sells wet suits that are at least 2% defective.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_suits_null`\n",
    "\n",
    "Complete the implementation of the function `simulate_suits_null`, which simulates a single step of the data generation process under the null hypothesis. The function should return a binary array, i.e. an array of 0s and 1s, of length 250. It is up to you to decide what the 0s and 1s mean.\n",
    "\n",
    "***Hints:*** `np.random.choice` might be useful in this case.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `estimate_p_val`\n",
    "\n",
    "Complete the implementation of the function `estimate_suits_p_val`, which takes in an integer `N` and returns the estimated p-value of your investigation upon simulating the null hypothesis `N` times.\n",
    "\n",
    "***Note***: Plot the null distribution and your observed statistic to check your work. (If you decide to plot, you may have to run `import matplotlib.pyplot as plt` or `import plotly.express as px`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten our feet wet with hypothesis testing, let's take a closer look at how to choose null and alternative hypotheses and test statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Tires üöó\n",
    "\n",
    "A tire manufacturer, TritonTire, claims that their tires are so good, they will bring a Toyota Highlander from 60 mph to a complete stop in under 106 feet, 97% percent of the time.\n",
    "\n",
    "Now, you own a Toyota Highlander equipped with TritonTire tires, and you decide to test this claim. You take your car to an empty Vons parking lot, speed up to exactly 60 mph, hit the brakes, and measure the stopping distance. As illegal as it is, you repeat this process 50 times and find that **you stopped in under 106 feet only 47 of the 50 times**.\n",
    "\n",
    "Livid, you call TritonTire and say that their claim is false. They say, no, that you were just unlucky: your experiment is consistent with their claim. But they didn't realize that they are dealing with a *data scientist* üßë‚Äçüî¨.\n",
    "\n",
    "To settle the matter, you decide to unleash the power of the hypothesis test. The following three subparts ask you to answer a total of four select-all multiple choice questions.\n",
    "\n",
    "#### Question 4.1\n",
    "\n",
    "You will set up a hypothesis test in order to test your suspicion that the tires are are actually worse than claimed. Which of the following are valid null and alternative hypotheses for this hypothesis test?\n",
    "\n",
    "1. The tires will stop your car in under 106 feet exactly 97% of the time.\n",
    "0. The tires will stop your car in under 106 feet less than 97% of the time.\n",
    "0. The tires will stop your car in under 106 feet greater than 97% of the time.\n",
    "0. The tires will stop your car in more than 106 feet exactly 3% of the time.\n",
    "0. The tires will stop your car in more than 106 feet less than 3% of the time.\n",
    "0. The tires will stop your car in more than 106 feet greater than 3% of the time.\n",
    "\n",
    "Complete the implementation of the function `car_null_hypoth`, which takes zero arguments and returns a list of integers, corresponding to the the valid null hypotheses above.\n",
    "Also complete the implementation of the function called `car_alt_hypoth`, which takes zero arguments and returns a list of integers, corresponding to the valid alternative hypotheses above.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Question 4.2\n",
    "\n",
    "Which of the following are valid test statistics for our question?\n",
    "\n",
    "1. The number of times the car stopped in under 106 feet in 50 attempts.\n",
    "1. The average number of feet the car took to come to a complete stop in 50 attempts.\n",
    "1. The number of attempts it took before the car stopped in under 95 feet.\n",
    "1. The proportion of attempts in which the car stopped in under 106 feet in 50 attempts.\n",
    "\n",
    "Complete the implementation of the function `car_test_stat`, which takes zero arguments and returns a list of integers, corresponding to the valid test statistics above\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Question 4.3\n",
    "\n",
    "The p-value is the probability, under the assumption the null hypothesis is true, of observing a test statistic **equal to our observed statistic, or more extreme in the direction of the alternative hypothesis**.\n",
    "\n",
    "Why don't we just look at the probability of observing a test statistic equal to our observed statistic? That is, why is the \"more extreme in the direction of the alternative hypothesis\" part necessary?\n",
    "\n",
    "1. Because our observed test statistic isn't extreme.\n",
    "4. Because our null hypothesis isn't suggesting equality.\n",
    "5. Because our alternative hypothesis isn't suggesting equality.\n",
    "2. Because the probability of finding our observed test statistic equals the probability of finding something more extreme.\n",
    "3. Because if we run more and more trials (where a trial is speeding up the car then stopping), the probability of finding *any* particular observed test statistic gets closer and closer to zero, so if we did this we would always reject the null with more trials even if the null is true. For example, flipping a fair coin twice means it‚Äôs pretty likely to see 50% heads, but flipping it 10000 times means it‚Äôs quite unlikely to see 50% heads.\n",
    "\n",
    "Complete the implementation of the function `car_p_value`, which takes zero arguments and returns the correct reason as an integer (not a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 ‚Äì Superheroes ü¶∏\n",
    "\n",
    "In the previous two questions, we ran hypothesis tests that didn't require us to look at stored data. In this next question, we'll return to the `heroes` DataFrame from Lab 2, which is read in from the file `data/superheroes.csv`.\n",
    "\n",
    "Our goal in this section will be to answer the question:\n",
    "\n",
    "> Are blond-haired, blue-eyed characters significantly **more** \"good\" than the general pool of characters?\n",
    "\n",
    "#### `bhbe_col`\n",
    "\n",
    "To start, complete the implementation of the function `bhbe_col`, which takes in a DataFrame like `heroes` and returns a Boolean Series that contains `True` for characters that have **both** blond hair and blue eyes, and `False` for all other characters. \n",
    "\n",
    "***Note***: If a character's hair color contains the word `'blond'`, uppercase or lowercase, we consider their hair to be blond for the purposes of this question. Similarly, if a character's eye color contains the word `'blue'`, uppercase or lowercase, we consider their eye color to be blue for the purposes of this question.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that you have an easy way of accessing only the blond-haired, blue-eyed characters in `heroes`, you can proceed with a hypothesis test. You choose the following null hypothesis:\n",
    "\n",
    "> The proportion of \"good\" characters among blond-haired, blue-eyed characters is equal to the proportion of \"good\" characters in the overall population.\"\n",
    "\n",
    "Fix a significance level (i.e. p-value cutoff) of 1%.\n",
    "\n",
    "Before proceeding, think about what test statistic to use in this hypothesis test (and to do that, read the initial question carefully). Once you've done that, complete the implementations of the following functions.\n",
    "\n",
    "***Hint:*** Alternative hypothesis: the distribution of \"good\" characters among blond-haired, blue-eyed characters is different from the proportion of \"good\" characters in the overall population.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `superheroes_observed_stat`\n",
    "Complete the implementation of the function `superheroes_observed_stat`, which takes in the DataFrame `heroes` and returns the observed test statistic.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_bhbe_null` \n",
    "Complete the implementation of the function `simulate_bhbe_null`, which takes in a positive integer `n` and returns an array of length `n`, where each element is a simulated test statistic according to the null hypothesis. You should hard-code the simulation parameter within your function; do not read in any data. (The simulation parameter is a proportion/probability; you can round it to two decimal places.)\n",
    "\n",
    "***Hint:*** While you're not prohibited from using a `for`-loop, try avoiding one here. You can access columns of a multidimensional array the same way you access columns of a DataFrame using `iloc`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `superheroes_calc_pval` \n",
    "Complete the implementation of the function `superheroes_calc_pval`, which takes in no parameters and returns a list where:\n",
    "* The first element is the p-value for the hypothesis test (using 100,000 simulations). Please run the code yourself **in your notebook** and hard-code this answer **in your `.py` file**, as actually running the 100,000 simulation hypothesis test will timeout on Gradescope.\n",
    "* The second element is `'Reject'` if you reject the null hypothesis and `'Fail to reject'` if you fail to reject the null hypothesis, at the 1% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = Path('data')/'superheroes.csv'\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "bhbe_out = bhbe_col(heroes)\n",
    "\n",
    "obs_stat_out = superheroes_observed_stat(heroes)\n",
    "\n",
    "simulate_bhbe_out = simulate_bhbe_null(10)\n",
    "\n",
    "pval_out = superheroes_calc_pval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Permutation Testing\n",
    "\n",
    "Recall, hypothesis tests answer questions of the form:\n",
    "\n",
    "> I have a population distribution, and I have one sample. Does this sample look like it was drawn from the population?\n",
    "\n",
    "While permutation tests answer questions of the form:\n",
    "\n",
    "> I have two samples, but no information about any population distributions. Do these samples look like they were drawn from the same population?\n",
    "\n",
    "Keep this in mind while working on this last part of the lab.\n",
    "\n",
    "<br>\n",
    "\n",
    "[Skittles](https://en.wikipedia.org/wiki/Skittles_(confectionery)) üç¨ are made in two locations in the United States: Yorkville, Illinois and Waco, Texas. In these factories, Skittles of different colors are made separately by different machines and combined/packaged into bags for sale. The **tab-separated file** `data/skittles.tsv` contains the contents of 468 bags of Skittles.\n",
    "\n",
    "Throughout this question, we will compare the color distribution of Skittles between bags made in the Yorkville factory and bags made in the Waco factory. Most people have preferences for their favorite flavor, and there is a surprising amount of variation among the distribution of flavors in each bag.\n",
    "\n",
    "Look at the variation by bag in the dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skittles_fp = Path('data')/'skittles.tsv'\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\t')\n",
    "skittles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skittles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 ‚Äì Orange Skittles üü†\n",
    "\n",
    "First, you will investigate if the machine that mixes together the Skittles of different colors might favor one color over another. Use a permutation test to assess whether, on average, bags made in Yorkville have the same number of orange skittles as bags made in Waco. Do this by implementing the functions described below.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `diff_of_means`\n",
    "\n",
    "Complete the implementation of the function `diff_of_means`, which takes in a DataFrame like `skittles` and returns the **absolute difference** between the **mean** number of orange Skittles per bag from Yorkville and the **mean** number of orange Skittles per bag from Waco.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_null`\n",
    "\n",
    "Complete the implementation of the function `simulate_null`, which takes in a DataFrame like `skittles` and returns one simulated instance of the test statistic under the null hypothesis. Note that this will involve shuffling the `'Factory'` column!\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `pval_color`\n",
    "\n",
    "Complete the implementation of the function `pval_color`, which takes in a DataFrame like `skittles` and calculates the p-value for the permutation test using 1000 trials.\n",
    "\n",
    "<br>\n",
    "\n",
    "Plot the observed statistic, along with the histogram for the simulated distribution, to check your work.\n",
    "\n",
    "***Note:*** In all functions, the default argument for `col` is `'orange'`. Your functions should still work for any color so that you can call it in later questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "# cell may take about 1-2 minutes to execute to completion\n",
    "skittles_fp = Path('data')/'skittles.tsv'\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\\\t', engine='python')\n",
    "q6_diff_of_means_out = diff_of_means(skittles)\n",
    "q6_simulate_null_out = simulate_null(skittles)\n",
    "q6_pval_out = pval_color(skittles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 ‚Äì Generalizing to all colors üî¥üü†üü°üü¢üü£\n",
    "\n",
    "While your `pval_color` function used a default color of `'orange'`, it should also work for all other colors of Skittles, meaning you can run the same permutation test from Question 7 on all colors of Skittles. Call `pval_color` on all colors of Skittles to find which colors differ the most between the two locations on average. \n",
    "\n",
    "Then, complete the implementation of the function `ordered_colors`, which returns a list of five ordered pairs, each of the form `('color', p_value)`. For example, your list might look like `[('pink', 0.000), ('brown', 0.025), ...]`. \n",
    "\n",
    "The list should be **hard-coded**, meaning that you should run your permutation tests in your notebook, not in your `.py` file. The list should also be sorted in **increasing order of p-value**. Make sure your p-values are rounded to **3 decimal places**.\n",
    "\n",
    "Even though there is randomness in the color composition in each bag, this list gives the likelihood that the machines have a systematic, meaningful, difference in how they blend the colors in each bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q7_out = ordered_colors()\n",
    "q7_colors = {'green', 'orange', 'purple', 'red', 'yellow'}\n",
    "q7_test_colors = [x[0] for x in q7_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 ‚Äì Overall distributions üè≠\n",
    "\n",
    "Now, suppose you would like to assess whether the two locations make similar amounts of each color overall. That is, suppose we:\n",
    "* Combine and count up all the Skittles of each color that were made in Yorkville (e.g. 14303 total red skittles, 9091 total green skittles, etc.).\n",
    "* Combine and count up all the Skittles of each color that were made in Waco.\n",
    "\n",
    "**Are these distributions of colors similar?** Is the variation among the bags due to each factory making different amounts of each color?\n",
    "\n",
    "Use a permutation test to assess whether the distribution of colors of Skittles made in Yorkville is statistically significantly different than those made in Waco. Set a significance level (i.e. p-value cutoff) of 0.01 and determine whether you can reject a null hypothesis that answers the question above using a permutation test with 1000 trials. For your test statistic, use the **total variation distance (TVD)**.\n",
    "\n",
    "Refer to [Lecture 6](https://dsc80.com/resources/lectures/lec06/lec06.html) to see an example of a [permutation test](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html) that uses the [TVD](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html) as the test statistic. Some guidance:\n",
    "\n",
    "- Our previous permutation tests have compared the mean number of (say) orange Skittles in Yorkville bags to the mean number number of orange Skittles in Waco bags. The role of shuffling was to randomly assign bags to Yorkville and Waco.\n",
    "- In this permutation test, we are **still** shuffling to randomly assign bags to Yorkville and Waco. The only difference is that after we randomly assign each bag to a factory, we will compute the **distribution** of colors among the two factories and find the TVD between those two distributions.\n",
    "\n",
    "**Your job:** Complete the implementation of the function `same_color_distribution`, which takes in no arguments and outputs a hard-coded **tuple** with the p-value and whether you `'Fail to Reject'` or `'Reject'` the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q8_out = same_color_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 ‚Äì Permutation testing vs. hypothesis testing üß™\n",
    "\n",
    "In each of the following scenarios, decide  whether  a  permutation test is appropriate to determine if there is a  significant difference between the quantities described. If a permutation test is appropriate, mark `'P'`. Otherwise, mark `'H'`.\n",
    "\n",
    "Record your answers in the function `perm_vs_hyp` that outputs a list of length 5, containing the values `'P'` and `'H'`.\n",
    "\n",
    "1. Compare the DSC 80 pass rate between second years and third years who take the class.\n",
    "2. Compare the proportion of Data Science majors who have completed DSC 80 and the proportion of Data Science minors who have completed DSC 80.\n",
    "3. Compare the proportion of students who have iPhones to the proportion of students who have Android phones (for simplicity, assume that all students either have an iPhone or an Android).\n",
    "4. In DSC 80, we ask all students whether they liked DSC 40A or DSC 40B more. Compare the proportion of students who preferred DSC 40A to the proportion who preferred DSC 40B.\n",
    "5. Compare the attendance rate of classes that play music before class vs. classes that do not play music before class.\n",
    "\n",
    "***Hint:*** Think about the type of data you would collect in each case, and how you would simulate new data under the null hypothesis. It will be useful to refer to the explanation at the start of Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q9_out = perm_vs_hyp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done with Lab 4! üèÅ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To verify that all of your work is indeed in `lab.py`, and that you didn't accidentally implement a function in this notebook and not in `lab.py`, we've included another notebook in the lab folder, called `lab-validation.ipynb`. `lab-validation.ipynb` is a version of this notebook with only the `grader.check` cells and the code needed to set up the tests. \n",
    "\n",
    "### **Go to `lab-validation.ipynb`, and go to Kernel > Restart & Run All.** This will check if all `grader.check` test cases pass using just the code in `lab.py`.\n",
    "\n",
    "Once you're able to pass all test cases in `lab-validation.ipynb`, including the call to `grader.check_all()` at the very bottom, then you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1_result) == 433\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q1_result.loc[393, \"Time\"] > 9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q1_result.loc[457, 'Time'] == 4\nTrue",
         "failure_message": "check user 457",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q1_result.loc[458, 'Time'] > 50\nTrue",
         "failure_message": "check user 457",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> len(set(q1_result.index)) == 433\nTrue",
         "failure_message": "check number of unique users",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q2_result) == 433\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> np.isclose(q2_result.loc[466], 0.24250681198910082)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(suits_null_hyp(), list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(suits_null_hyp()).issubset({1, 2, 3, 4})\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> pd.Series(simulate_suits_null()).isin([0, 1]).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0 < estimate_suits_p_val(1000) < 0.1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(car_null_hypoth()) <= set(range(1, 7))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set(car_alt_hypoth()) <= set(range(1, 7))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set(car_test_stat()) <= set(range(1, 5))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> car_p_value() in set(range(1, 6))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(bhbe_out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bhbe_out.dtype == np.dtype('bool')\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bhbe_out.sum() == 93\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0.5 <= obs_stat_out <= 1.0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(simulate_bhbe_out, np.ndarray)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> simulate_bhbe_out.shape[0] == 10\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ((0.45 <= simulate_bhbe_out) & (simulate_bhbe_out <= 1)).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(pval_out) == 2\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0 <= pval_out[0] <= 1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> pval_out[1] in ['Reject', 'Fail to reject']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q6_diff_of_means_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q6_simulate_null_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> 0 <= q6_simulate_null_out <= 1.0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q6_pval_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> 0 <= q6_pval_out <= 1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q6_diff_of_means_out > 0\nTrue",
         "failure_message": "should be greater than zero",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q7_out) == 5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set([x[0] for x in q7_out]) == q7_colors\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> all([isinstance(x[1], float) for x in q7_out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q7_test_colors.index('green') > q7_test_colors.index('yellow')\nTrue",
         "failure_message": "yellow less than green",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q7_out[0][1], 0.0)\nTrue",
         "failure_message": "smallest pval",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q8_out, tuple)\nTrue",
         "failure_message": "tuple",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q8_out[0], float)\nTrue",
         "failure_message": "wrong output type at index 0",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q8_out[1] in ['Fail to Reject', 'Reject']\nTrue",
         "failure_message": "wrong output type at index 1",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q8_out[0], 0.005, atol=0.25)\nTrue",
         "failure_message": "p-value, approximate within 0.25",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q8_out[0], 0.005, atol=0.5)\nTrue",
         "failure_message": "p-value, approximate within 0.5",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q9_out) == 5\nTrue",
         "failure_message": "output length should be 5",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(q9_out) <= set(['P', 'H'])\nTrue",
         "failure_message": "output contains answers other than P or H",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
