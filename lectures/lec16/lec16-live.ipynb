{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a44b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from dsc80_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c820354",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import lec16_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca51859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 16 ‚Äì More Generalization, Decision Trees\n",
    "\n",
    "## DSC 80, Fall 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22824200",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üì£ Announcements üì£\n",
    "\n",
    "- Project 4 due tomorrow!\n",
    "- Lab 9 out, due Dec 4.\n",
    "- Final Exam on Mon, Dec 11, 3-6pm in WLH 2005 (our usual lecture room).\n",
    "    - Two cheat sheets allowed (feel free to reuse your midterm sheet).\n",
    "    - More details to come."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd851d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üìÜ Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eb093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üôãüôãüèΩ‚Äç‚ôÄÔ∏è Slido\n",
    "\n",
    "https://app.sli.do/event/2LZSnXWNpGPiuVnCZMa5J8\n",
    "\n",
    "<center><img src=\"imgs/slido.svg\" width=\"30%\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad5ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice Exam Question ü§î\n",
    "\n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29f107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39134087",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "\n",
    "def sample_dgp(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "sample_1 = sample_dgp()\n",
    "sample_2 = sample_dgp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in lec15_util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6b397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance\n",
    "\n",
    "The training data we have access to is a sample from the DGP. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same DGP.\n",
    "\n",
    "Suppose we **fit** a model $H$ (e.g. a degree 3 polynomial) on **several different datasets** from a DGP. There are three sources of error that arise:\n",
    "\n",
    "* ‚≠êÔ∏è **Model Bias**: **The expected deviation between a predicted value and an actual value**.\n",
    "    - In other words, **for a given $x_i$, how far is $H(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ‚úÖ\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response.\n",
    "\n",
    "- ‚≠êÔ∏è **Model variance (\"variance\")**: **The variance of a model's predictions**.\n",
    "    - In other words, **for a given $x_i$, what is the variance of $H(x_i)$ across all datasets**?\n",
    "    - Low model variance is good! ‚úÖ\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data.\n",
    "\n",
    "- **Observation variance**: The variance due to the random noise in the process we are trying to model (e.g. measurement error). _We can't control this, without collecting more data!_\n",
    "\n",
    "- (See hand-written notes from lecture for more detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f7217",
   "metadata": {},
   "source": [
    "### Implications of Bias and Variance\n",
    "\n",
    "- Risk: $ R(H) = \\text{bias}^2 + \\text{variance} + \\text{irreducible error} $\n",
    "\n",
    "**Model Fit:**\n",
    "\n",
    "- Underfitting = too much bias\n",
    "- Most overfitting = too much variance\n",
    "- Training error reflects bias but **not variance**.\n",
    "- Test error reflects **both bias and variance**.\n",
    "\n",
    "**As $n$ increases:**\n",
    "\n",
    "- Generally, $ n\\uparrow $ means variance $ \\downarrow $.\n",
    "- If $ H(x) $ can fit the true DGP exactly, then $ n\\uparrow $ means bias $ \\downarrow $.\n",
    "    - For certain loss functions (e.g. MSE), bias will be 0 if $ H(x) $ can fit the true DGP exactly.\n",
    "- If $ H(x) $ cannot fit the true DGP well, then bias large for most points.\n",
    "\n",
    "**As we add more features:**\n",
    "\n",
    "- Adding a useful feature reduces bias.\n",
    "- Adding a useless feature doesn't change bias.\n",
    "- Adding feature generally increases variance, even if it's useless.\n",
    "\n",
    "**In real life:**\n",
    "\n",
    "- Don't usually know the true DGP, so we can't put actual numbers to bias-variance decomposition.\n",
    "- Train-test split so we can estimate $ R(h) $ using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ddf34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Linear Regression\n",
    "\n",
    "- If actual DGP is a linear model:\n",
    "- Bias = 0.\n",
    "- Variance $ \\propto \\frac{d}{n} $, where $ d $ is the dimension (number of features) per sample point.\n",
    "    - $ n \\uparrow $ = variance $ \\downarrow $\n",
    "    - $ d \\uparrow $ = variance $ \\uparrow $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9b6b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Generalization\n",
    "\n",
    "1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>.\n",
    "\n",
    "2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "    - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose hyperparameters and estimate the model's ability to generalize.\n",
    "    - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!\n",
    "    \n",
    "3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set.\n",
    "\n",
    "4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. RMSE) is not acceptable, return to step 2.\n",
    "\n",
    "5. Finally, train on **all available data** and ship the model to production! üõ≥\n",
    "\n",
    "üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7a7b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üôãüôãüèΩ‚Äç‚ôÄÔ∏è Questions?\n",
    "\n",
    "https://app.sli.do/event/2LZSnXWNpGPiuVnCZMa5J8\n",
    "\n",
    "<center><img src=\"imgs/slido.svg\" width=\"30%\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72e409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees üå≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6646d38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/ml-taxonomy.svg' width=50%></center>\n",
    "\n",
    "Although decision trees can be used for both regression and classification, we'll be using them for **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f1289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Should I get groceries?\n",
    "\n",
    "<div style=\"display: flex;\"><img src='imgs/dtree-basic.svg' width=30%/><img src='imgs/dtree-basic-plot.svg' width=30%></div>\n",
    "\n",
    "- Internal nodes of tree check feature values.\n",
    "- Leaf nodes of tree specify class $H(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569dd49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83270fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "display_df(diabetes, cols=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c4fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0 means no diabetes, 1 means yes diabetes.\n",
    "diabetes['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d4eb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'Glucose'` is measured in mg/dL (milligrams per deciliter).\n",
    "\n",
    "- `'BMI'` is calculated as $\\text{BMI} = \\frac{\\text{weight (kg)}}{\\left[ \\text{height (m)} \\right]^2}$.\n",
    "\n",
    "- Let's use `'Glucose'` and `'BMI'` to predict whether or not a patient has diabetes (`'Outcome'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d2b73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploring the dataset\n",
    "\n",
    "First, a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a446c5d",
   "metadata": {},
   "source": [
    "<span style='color: orange'><b>Class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4445bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    X_train.assign(Outcome=y_train.astype(str))\n",
    "            .plot(kind='scatter', x='Glucose', y='BMI', color='Outcome', \n",
    "                  color_discrete_map={'0': 'orange', '1': 'blue'},\n",
    "                  title='Relationship between Glucose, BMI, and Diabetes')\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f94087",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a decision tree\n",
    "\n",
    "Let's build a decision tree and interpret the results.\n",
    "\n",
    "The relevant class is `DecisionTreeClassifier`, from `sklearn.tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12902788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b06503",
   "metadata": {},
   "source": [
    "Note that we `fit` it the same way we `fit` earlier estimators.\n",
    "\n",
    "_You may wonder what `max_depth` and `criterion` do ‚Äì more on this soon!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5efa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2, criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4fc97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing decision trees\n",
    "\n",
    "Our fit decision tree is like a \"flowchart\", made up of a series of questions.\n",
    "\n",
    "As before, <span style='color: orange'><b>orange is \"no diabetes\"</b></span> and <span style='color: blue'><b>blue  is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plot_tree(dt, feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "          filled=True, fontsize=15, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f6931",
   "metadata": {},
   "source": [
    "- To **classify a new data point**, we start at the top and answer the first question (i.e. \"Glucose <= 129.5\").\n",
    "- If the answer is \"**Yes**\", we move to the **left** branch, otherwise we move to the right branch.\n",
    "- We repeat this process until we end up at a leaf node, at which point we predict the most common class in that node.\n",
    "    - Note that each node has a `value` attribute, which describes the number of **training** individuals of each class that fell in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the left node at depth 2 has a `value` of [304, 78].\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73e4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating classifiers\n",
    "\n",
    "The most common evaluation metric in classification is **accuracy**:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{# data points classified correctly}}{\\text{# data points}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d61d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `score` method of a classifier computes accuracy by default (just like the `score` method of a regressor computes $R^2$ by default). We want our classifiers to have **high accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bde429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy ‚Äì same number as above\n",
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4045e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce926211",
   "metadata": {},
   "source": [
    "### About decision trees\n",
    "\n",
    "- Can work with categorical data too without using one-hot encoding.\n",
    "- Interpretable predictions.\n",
    "- Decision boundary can be arbitrarily complicated.\n",
    "- Works with multi-class classification (e.g. more than 2 possible outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda55de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we train?\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "```python\n",
    "def make_tree(X, y):\n",
    "    if all points in y have the same label C:\n",
    "        return Leaf(C)\n",
    "    f = best splitting feature # e.g. Glucose or BMI\n",
    "    v = best splitting value   # e.g. 129.5\n",
    "    \n",
    "    X_left, y_left   = X, y where (X[f] < v)\n",
    "    X_right, y_right = X, y where (X[f] >= v)\n",
    "    \n",
    "    left  = make_tree(X_left, y_left)\n",
    "    right = make_tree(X_right, y_right)\n",
    "    \n",
    "    return Node(f, v, left, right)\n",
    "    \n",
    "make_tree(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911326e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we decide on the best split?\n",
    "\n",
    "- Choose a loss function $ L(X, y) $.\n",
    "- Try all splits, then pick the one that minimizes $ L(X_{\\text{left}}, y_{\\text{left}}) + L(X_{\\text{right}}, y_{\\text{right}}) $.\n",
    "- What's a good $ L(X, y) $?\n",
    "\n",
    "**Intuition:** Suppose the distribution within a node looks like this (colors represent classes):\n",
    "\n",
    "<center>üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµüîµüîµ</center>\n",
    "\n",
    "Split A:\n",
    "- \"Yes\": üü†üü†üü†üîµüîµüîµ\n",
    "- \"No\": üü†üü†üü†üîµüîµüîµüîµ\n",
    "\n",
    "Split B:\n",
    "- \"Yes\": üîµüîµüîµüîµüîµüîµ\n",
    "- \"No\": üîµüü†üü†üü†üü†üü†üü†\n",
    "\n",
    "**Which split is \"better\"?**\n",
    "\n",
    "Split B, because there is \"less uncertainty\" in the resulting nodes in split B than there is in split A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7c118",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One (bad) idea:\n",
    "\n",
    "- Label a node with the majority class $ C $.\n",
    "- $ L(X, y) $ = number of points where $ y \\neq C $.\n",
    "\n",
    "Why is this bad? Suppose we have:\n",
    "\n",
    "<center>üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµüîµ</center>\n",
    "\n",
    "Split A:\n",
    "- \"Yes\": üü†üü†üü†üü†üü†üü†üîµ\n",
    "- \"No\": üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµ\n",
    "\n",
    "Split B:\n",
    "- \"Yes\": üü†üü†üü†üü†üü†üü†üîµüîµüîµ\n",
    "- \"No\": üü†üü†üü†üü†üü†üü†üîµüîµüîµ\n",
    "\n",
    "We prefer Split A, but $ L(X, y) = 6 $ for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8416f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A better idea: entropy\n",
    "\n",
    "- For each label $C$ within a node, define $p_C$ as the proportion of points with the label.\n",
    "- The **surprise** of drawing a point from the node at random and having it be class $C$ is:\n",
    "\n",
    "$$\n",
    "- \\log_2 p_C\n",
    "$$\n",
    "\n",
    "- And the **entropy** of a node is the average surprise over all classes:\n",
    "\n",
    "\\begin{align}\n",
    "L(X, y) = - \\sum_C p_C \\log_2 p_C\n",
    "\\end{align}\n",
    "\n",
    "- The entropy of üü†üü†üü†üü†üü†üü†üü†üü† is $ -1 \\log_2(1) = 0 $.\n",
    "- The entropy of üü†üü†üü†üü†üîµüîµüîµüîµ is $ -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1 $.\n",
    "- The entropy of üü†üîµüü¢üü°üü£ = $ - \\log_2 \\frac{1}{5} = \\log_2(5) $\n",
    "    - In general, if there are $n$ points all with different labels, entropy = $ \\log_2(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4726d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy Example\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "<center>üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµüîµ</center>\n",
    "\n",
    "Split A:\n",
    "- \"Yes\": üü†üü†üü†üü†üü†üü†üîµ\n",
    "- \"No\": üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµ\n",
    "\n",
    "Split B:\n",
    "- \"Yes\": üü†üü†üü†üü†üü†üü†üîµüîµüîµ\n",
    "- \"No\": üü†üü†üü†üü†üü†üü†üîµüîµüîµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    props = pd.Series(list(labels)).value_counts() / len(labels)\n",
    "    return -sum(props * np.log2(props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_a = entropy(\"üü†üü†üü†üü†üü†üü†üîµ\") + entropy(\"üü†üü†üü†üü†üü†üü†üîµüîµüîµüîµüîµ\")\n",
    "split_b = entropy(\"üü†üü†üü†üü†üü†üü†üîµüîµüîµ\") + entropy(\"üü†üü†üü†üü†üü†üü†üîµüîµüîµ\")\n",
    "split_a, split_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1887c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Split A has lower entropy, so we'll pick it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe85034",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üôãüôãüèΩ‚Äç‚ôÄÔ∏è Questions?\n",
    "\n",
    "https://app.sli.do/event/2LZSnXWNpGPiuVnCZMa5J8\n",
    "\n",
    "<center><img src=\"imgs/slido.svg\" width=\"30%\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e27f0",
   "metadata": {},
   "source": [
    "### Runtime (optional)\n",
    "\n",
    "- Predict a point: traverse tree until leaf.\n",
    "    - Runtime is $ O(\\text{tree depth}) $.\n",
    "    - If all features are binary (two categories), then tree depth ‚â§ $d$ (number of features).\n",
    "    - Usually ‚â§ $O(\\log n)$ , but not always.\n",
    "\n",
    "- Training:\n",
    "    - For binary features, need to try $ O(d) $ splits at each node\n",
    "    - For numeric features, there's a way to check all splits in $ O(n') $ time, where $ n' $ is the number of points in the node. Since there can be $ d $ numeric features, overall runtime is $ O(n'd) $ for each node.\n",
    "    - Each point is used in $ O(\\text{depth}) $ nodes, so overall runtime to fit is $ O(nd \\cdot \\text{depth}) $.\n",
    "    - Since depth is often logarithmic, runtime is pretty fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63715f3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tree depth\n",
    "\n",
    "Decision trees are trained by **recursively** picking the best split until:\n",
    "\n",
    "- all \"leaf nodes\" only contain training examples from a single class (good), or\n",
    "- it is impossible to split leaf nodes any further (not good).\n",
    "\n",
    "By default, there is no \"maximum depth\" for a decision tree. As such, without restriction, decision trees tend to be very deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max = DecisionTreeClassifier()\n",
    "dt_no_max.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7ab31",
   "metadata": {},
   "source": [
    "A decision tree fit on our training data has a depth of around 20! (It is so deep that `tree.plot_tree` errors when trying to plot it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c06bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30caeb75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At first, this tree seems \"better\" than our tree of depth 2, since its training accuracy is much much higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 2 tree.\n",
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But recall, we truly care about **test set performance**, and this decision tree has **worse accuracy on the test set than our depth 2 tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f35a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 2 tree.\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1e183",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision trees and overfitting\n",
    "\n",
    "- Decision trees have a tendency to overfit. **Why is that?**\n",
    "\n",
    "- Unlike linear classification techniques (like logistic regression or SVMs), **decision trees are non-linear**.\n",
    "    - They are also \"non-parametric\" ‚Äì there are no $w^*$s to learn.\n",
    "\n",
    "- While being trained, decision trees ask enough questions to effectively **memorize** the correct response values in the training set. However, the relationships they learn are often overfit to the noise in the training set, and don't generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e165b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f563e29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A decision tree whose depth is not restricted will achieve 100% accuracy on any training set, as long as there are no \"overlapping values\" in the training set.\n",
    "    - Two values overlap when they have the same features $x$ but different response values $y$ (e.g. if two patients have the same glucose levels and BMI, but one has diabetes and one doesn't).\n",
    "\n",
    "- **One solution**: Make the decision tree \"less complex\" by limiting the maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87429ef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since `sklearn.tree`'s `plot_tree` can't visualize extremely large decision trees, let's create and visualize some smaller decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66549935",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = {}\n",
    "for d in [2, 4, 8]:\n",
    "    trees[d] = DecisionTreeClassifier(max_depth=d, random_state=1)\n",
    "    trees[d].fit(X_train, y_train)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5), dpi=100)\n",
    "    plot_tree(trees[d], feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "               filled=True, rounded=True, impurity=False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c504a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As tree depth increases, complexity increases, and our trees are more prone to overfitting.\n",
    "\n",
    "**Question**: What is the \"right\" maximum depth to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ba76e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters for decision trees\n",
    "\n",
    "- `max_depth` is a hyperparameter for `DecisionTreeClassifier`.\n",
    "\n",
    "- There are many more hyperparameters we can tweak; look at [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for examples.\n",
    "    - `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - `criterion`: The function to measure the quality of a split (`'gini'` or `'entropy'`).\n",
    "\n",
    "- To ensure that our model generalizes well to unseen data, we need an efficient technique for trying different combinations of hyperparameters!\n",
    "\n",
    "#### Thinking about bias and variance\n",
    "\n",
    "- Bigger `max_depth` = less bias, more variance.\n",
    "- Bigger `min_samples_split` = more bias, less variance. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8a664",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üôãüôãüèΩ‚Äç‚ôÄÔ∏è Questions?\n",
    "\n",
    "https://app.sli.do/event/2LZSnXWNpGPiuVnCZMa5J8\n",
    "\n",
    "<center><img src=\"imgs/slido.svg\" width=\"30%\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c9855",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64145bef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid search\n",
    "\n",
    "`GridSearchCV` takes in:\n",
    "- an **un-`fit`** instance of an estimator, and\n",
    "- a **dictionary** of hyperparameter values to try,\n",
    "\n",
    "and performs $k$-fold cross-validation to find the **combination of hyperparameters with the best average validation performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eefce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e2ad6",
   "metadata": {},
   "source": [
    "The following dictionary contains the values we're considering for each hyperparameter. (We're using `GridSearchCV` with 3 hyperparameters, but we could use it with even just a single hyperparameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth': [2, 3, 4, 5, 7, 10, 13, 15, 18, None], \n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa809697",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that there are 140 **combinations** of hyperparameters we need to try. We need to find the **best combination** of hyperparameters, not the best value for each hyperparameter individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyperparameters['max_depth']) * \\\n",
    "len(hyperparameters['min_samples_split']) * \\\n",
    "len(hyperparameters['criterion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4388c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`GridSearchCV` needs to be instantiated and `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27641085",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After being `fit`, the `best_params_` attribute provides us with the best combination of hyperparameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a75507",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58245cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All of the intermediate results ‚Äì validation accuracies for each fold, mean validation accuaries, etc. ‚Äì are stored in the `cv_results_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb33f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searcher.cv_results_['mean_test_score'] # Array of length 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd84274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows correspond to folds, columns correspond to hyperparameter combinations.\n",
    "pd.DataFrame(np.vstack([searcher.cv_results_[f'split{i}_test_score'] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d82df9",
   "metadata": {},
   "source": [
    "Note that the above DataFrame tells us that 5 * 140 = 700 models were trained in total!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e712c39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've found the best combination of hyperparameters, we should fit a decision tree instance using those hyperparameters on our entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f566b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree = DecisionTreeClassifier(**searcher.best_params_)\n",
    "final_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy.\n",
    "final_tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy.\n",
    "final_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbae18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember, `searcher` itself is a model object (we had to `fit` it). After performing $k$-fold cross-validation, behind the scenes, `searcher` is trained on the entire training set using the optimal combination of hyperparameters.\n",
    "\n",
    "In other words, `searcher` makes the same predictions that `final_tree` does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c59628",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing possible hyperparameter values\n",
    "\n",
    "- A full grid search can take a **long time**.\n",
    "    - In our previous example, we tried 140 combinations of hyperparameters.\n",
    "    - Since we performed 5-fold cross-validation, we trained 700 decision trees under the hood.\n",
    "\n",
    "- **Question**: How do we pick the possible hyperparameter values to try?\n",
    "\n",
    "- **Answer**: Trial and error.\n",
    "    - If the \"best\" choice of a hyperparameter was at an extreme, try increasing the range.\n",
    "    - For instance, if you try `max_depth`s from 32 to 128, and 32 was the best, try including `max_depths` under 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10d1a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- Decision trees are trained by finding the best questions to ask using the features in the training data. A good question is one that isolates classes as much as possible.\n",
    "- Decision trees have a tendency to overfit to training data. One way to mitigate this is by restricting the maximum depth of the tree.\n",
    "- To efficiently find hyperparameters through cross-validation, use `GridSearchCV`.\n",
    "    - Specify which values to try for each hyperparameter, and `GridSearchCV` will try all **unique combinations of hyperparameters** and return the combination with the best average validation performance.\n",
    "    - `GridSearchCV` is not the only solution ‚Äì see [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a5ccc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üôãüôãüèΩ‚Äç‚ôÄÔ∏è Questions?\n",
    "\n",
    "https://app.sli.do/event/2LZSnXWNpGPiuVnCZMa5J8\n",
    "\n",
    "<center><img src=\"imgs/slido.svg\" width=\"30%\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be1703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
